{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plantilla de desarrollo para primer examen parcial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pautas:**\n",
    "- La presente plantilla es un EJEMPLO de cómo ordenar el código de tu examen\n",
    "- Tienes la libertad DE AGREGAR todos los métodos y secciones en el examen que consideres necesarias\n",
    "- Realizar el desarrollo por medio de métodos, por ejemplo, ReadInfo(), TrainModel(), etc \n",
    "- Los métodos deberán de estar lo mas claro y modularizados que sea posible\n",
    "- Realizar la documentación de cada método por medio de comentarios y DocStrings \n",
    "- Deberás de utilizar un modelo de ML o algún ensamble de os mismos (SVC, DT, NB, KNN, etc)\n",
    "- Recuerda que puedes usar un split de los datos para entrenamiento y validación\n",
    "- Puedes revisar la documentación de Sklearn, o la librría que decidas utilizar para entender los parámetros de entrenamiento de los modelos\n",
    "- NO está permitido el uso de modelos de Deep Learning (DNN, CNN, LSTM, etc.) NI el uso de embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/eubgo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/eubgo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/eubgo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Librerías para manejo de Dataframes\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Librerías para trabajar con el texto\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Librerías para entrenar una máquina de soporte vectorial (Ejemplo)\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Librerías para trabajar con métricas\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import metrics\n",
    "\n",
    "# Additional libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí cargamos la información del DataSet de entrenamiento\n",
    "def ReadCorpus(path):\n",
    "    \"\"\"Este método lee los de datos del corpus y los pasa a un dataFrame\n",
    "\n",
    "    Args:\n",
    "        path (string): Ubicación del archivo de entrada (Corpus)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df.index = np.arange(1, len(df) + 1)\n",
    "    print(\"Elementos en el DataSet:\", len(df))\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrega lo que consideres necesario aquí"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserta lo que consideres necesario aquí, por ejemplo\n",
    "def Preprocess(df):\n",
    "    \"\"\"Método para preprocesar el texto\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): Dataframe a aplicar transformaciones\n",
    "\n",
    "    Returns:\n",
    "        dataframe: Dataframe transformado\n",
    "    \"\"\"\n",
    "    \n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].str.lower()\n",
    "\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    df[\"title\"] = df[\"title\"].apply(lambda x: \" \".join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "    df[\"title\"] = df[\"title\"].str.replace(\"[^\\w\\s]\", \"\")\n",
    "\n",
    "    df[\"title\"] = df[\"title\"].apply(nltk.word_tokenize)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    df[\"title\"] = df[\"title\"].apply(lambda x: [lemmatizer.lemmatize(word) for word in x if not word.isdigit()])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando TF-IDF Vectorizer\n",
    "tfidf_vect = TfidfVectorizer(ngram_range=(1,2))\n",
    "\n",
    "# Usando Random Forest\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "\n",
    "def TrainModel(X, y):\n",
    "    \"\"\"Este método realiza el entrenamiento del modelo (Ejemplo)\n",
    "\n",
    "    Args:\n",
    "        X (list): Lista con los textos a transformar\n",
    "        y (list): Lista con los valores de y (Salida)\n",
    "\n",
    "    Returns:\n",
    "        model: Modelo entrenado\n",
    "    \"\"\"\n",
    "    # Parámetros para GridSearchCV\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Entrenando el modelo con los mejores parámetros\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_model.fit(X, y)\n",
    "    return best_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValidateModel(Y_test, Predicciones):\n",
    "    # Impresión de matriz de confusión\n",
    "    # print(\"Matriz de confusión:\")\n",
    "    # print(confusion_matrix(Y_test, Predicciones))\n",
    "\n",
    "    # Impresión de procentaje de Accuracy del modelo\n",
    "    print(\"\\nAccuracy del modelo: \")\n",
    "    print(metrics.accuracy_score(Y_test, Predicciones))\n",
    "\n",
    "    # Impresión de las métricas para el modelo\n",
    "    print(\"\\nMétricas de evaluación:\")\n",
    "    print(classification_report(Y_test, Predicciones))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline de todo el proceso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elementos en el DataSet: 16823\n"
     ]
    }
   ],
   "source": [
    "# Cargamos la información y creamos un DataFrame\n",
    "path = 'DataSet para entrenamiento del modelo.csv'\n",
    "df = ReadCorpus(path)\n",
    "\n",
    "# Preprocesamiento\n",
    "df_pre = Preprocess(df)\n",
    "\n",
    "# Lectura y split de los datos\n",
    "X = df_pre['title'].values.tolist()\n",
    "X = [' '.join(tokens) for tokens in X]\n",
    "y = df_pre['label'].values.tolist()\n",
    "\n",
    "X = count_vect.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "model = TrainModel(X_train, y_train)\n",
    "\n",
    "# Impresión de métricas\n",
    "Predicciones = model.predict(X_test)\n",
    "ValidateModel(y_test, Predicciones)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardado de modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle para guardar modelos\n",
    "import pickle\n",
    "\n",
    "filename = \"model_Nombre_chido.pickle\"\n",
    "\n",
    "# Guardar el modelo\n",
    "pickle.dump(model, open(filename, \"wb\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba del modelo (Parte mas importante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Pipeline() missing 1 required positional argument: 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m# Prueba para calificación del examen\u001b[39;00m\n\u001b[1;32m     23\u001b[0m input_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mDataSetClickBait.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 24\u001b[0m Pipeline(input_file)\n",
      "\u001b[0;31mTypeError\u001b[0m: Pipeline() missing 1 required positional argument: 'model'"
     ]
    }
   ],
   "source": [
    "def Pipeline(input_file, model):\n",
    "    # Cargamos la información y creamos un DataFrame\n",
    "    df = ReadCorpus(path)\n",
    "\n",
    "    # Preprocesamiento\n",
    "    df_pre = Preprocess(df)\n",
    "\n",
    "    # Lectura y split de los datos\n",
    "    X = df_pre['title'].values.tolist()\n",
    "    y = df_pre['label'].values.tolist()\n",
    "\n",
    "    X = count_vect.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "    model = TrainModel(X_train, y_train)\n",
    "\n",
    "    # Impresión de métricas\n",
    "    Predicciones = model.predict(X_test)\n",
    "    ValidateModel(y_test, Predicciones)\n",
    "\n",
    "\n",
    "# Prueba para calificación del examen\n",
    "input_file = 'DataSetClickBait.csv'\n",
    "Pipeline(input_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_up",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
