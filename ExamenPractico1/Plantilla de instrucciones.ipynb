{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plantilla de desarrollo para primer examen parcial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pautas:**\n",
    "- La presente plantilla es un EJEMPLO de cómo ordenar el código de tu examen\n",
    "- Tienes la libertad DE AGREGAR todos los métodos y secciones en el examen que consideres necesarias\n",
    "- Realizar el desarrollo por medio de métodos, por ejemplo, ReadInfo(), TrainModel(), etc \n",
    "- Los métodos deberán de estar lo mas claro y modularizados que sea posible\n",
    "- Realizar la documentación de cada método por medio de comentarios y DocStrings \n",
    "- Deberás de utilizar un modelo de ML o algún ensamble de os mismos (SVC, DT, NB, KNN, etc)\n",
    "- Recuerda que puedes usar un split de los datos para entrenamiento y validación\n",
    "- Puedes revisar la documentación de Sklearn, o la librría que decidas utilizar para entender los parámetros de entrenamiento de los modelos\n",
    "- NO está permitido el uso de modelos de Deep Learning (DNN, CNN, LSTM, etc.) NI el uso de embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/eubgo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/eubgo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/eubgo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Librerías para manejo de Dataframes\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Librerías para trabajar con el texto\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Librerías para entrenar una máquina de soporte vectorial (Ejemplo)\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Librerías para trabajar con métricas\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import metrics\n",
    "\n",
    "# Additional libraries\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí cargamos la información del DataSet de entrenamiento\n",
    "def ReadCorpus(path):\n",
    "    \"\"\"Este método lee los de datos del corpus y los pasa a un dataFrame\n",
    "\n",
    "    Args:\n",
    "        path (string): Ubicación del archivo de entrada (Corpus)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df.index = np.arange(1, len(df) + 1)\n",
    "    print(\"Elementos en el DataSet:\", len(df))\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrega lo que consideres necesario aquí"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserta lo que consideres necesario aquí, por ejemplo\n",
    "def Preprocess(df):\n",
    "    \"\"\"Método para preprocesar el texto\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): Dataframe a aplicar transformaciones\n",
    "\n",
    "    Returns:\n",
    "        dataframe: Dataframe transformado\n",
    "    \"\"\"\n",
    "    ### Limpieza del texto: sopwords, minúsculas, tokenización, lematización, etc.\n",
    "    \n",
    "    # poner todo el texto en minúsculas\n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].str.lower()\n",
    "\n",
    "    # eliminar stopwords\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    df[\"title\"] = df[\"title\"].apply(lambda x: \" \".join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "    # eliminar signos de puntuación\n",
    "    df[\"title\"] = df[\"title\"].str.replace(\"[^\\w\\s]\", \"\")\n",
    "\n",
    "    # tokenizar\n",
    "    df[\"title\"] = df[\"title\"].apply(nltk.word_tokenize)\n",
    "\n",
    "    # lematizar\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # quietar numeros\n",
    "    df[\"title\"] = df[\"title\"].apply(lambda x: [lemmatizer.lemmatize(word) for word in x if not word.isdigit()])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo lo que necesites para entrenar tu modelo\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "model = LinearSVC()\n",
    "\n",
    "\n",
    "def TrainModel(X, y):\n",
    "    \"\"\"Este método realiza el entrenamiento del modelo (Ejemplo)\n",
    "\n",
    "    Args:\n",
    "        X (list): Lista con los textos a transformar\n",
    "        y (list): Lista con los valores de y (Salida)\n",
    "\n",
    "    Returns:\n",
    "        model: Modelo entrenado\n",
    "    \"\"\"\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValidateModel(Y_test, Predicciones):\n",
    "    # Impresión de matriz de confusión\n",
    "    # print(\"Matriz de confusión:\")\n",
    "    # print(confusion_matrix(Y_test, Predicciones))\n",
    "\n",
    "    # Impresión de procentaje de Accuracy del modelo\n",
    "    print(\"\\nAccuracy del modelo: \")\n",
    "    print(metrics.accuracy_score(Y_test, Predicciones))\n",
    "\n",
    "    # Impresión de las métricas para el modelo\n",
    "    print(\"\\nMétricas de evaluación:\")\n",
    "    print(classification_report(Y_test, Predicciones))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline de todo el proceso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elementos en el DataSet: 16823\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m X \u001b[39m=\u001b[39m df_pre[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     10\u001b[0m y \u001b[39m=\u001b[39m df_pre[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m---> 12\u001b[0m X \u001b[39m=\u001b[39m count_vect\u001b[39m.\u001b[39;49mfit_transform(X)\n\u001b[1;32m     13\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size \u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m)\n\u001b[1;32m     15\u001b[0m model \u001b[39m=\u001b[39m TrainModel(X_train, y_train)\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1375\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1376\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1377\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1378\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1379\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1380\u001b[0m             )\n\u001b[1;32m   1381\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1383\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1385\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1386\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1269\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1270\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1271\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1272\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[0;32m---> 68\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[1;32m     69\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Cargamos la información y creamos un DataFrame\n",
    "path = 'DataSet para entrenamiento del modelo.csv'\n",
    "df = ReadCorpus(path)\n",
    "\n",
    "# Preprocesamiento\n",
    "df_pre = Preprocess(df)\n",
    "\n",
    "# Lectura y split de los datos\n",
    "X = df_pre['title'].values.tolist()\n",
    "y = df_pre['label'].values.tolist()\n",
    "\n",
    "X = count_vect.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "model = TrainModel(X_train, y_train)\n",
    "\n",
    "# Impresión de métricas\n",
    "Predicciones = model.predict(X_test)\n",
    "ValidateModel(y_test, Predicciones)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardado de modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle para guardar modelos\n",
    "import pickle\n",
    "\n",
    "filename = \"model_Nombre_chido.pickle\"\n",
    "\n",
    "# Guardar el modelo\n",
    "pickle.dump(model, open(filename, \"wb\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba del modelo (Parte mas importante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Pipeline() missing 1 required positional argument: 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m# Prueba para calificación del examen\u001b[39;00m\n\u001b[1;32m     23\u001b[0m input_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mDataSetClickBait.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 24\u001b[0m Pipeline(input_file)\n",
      "\u001b[0;31mTypeError\u001b[0m: Pipeline() missing 1 required positional argument: 'model'"
     ]
    }
   ],
   "source": [
    "def Pipeline(input_file, model):\n",
    "    # Cargamos la información y creamos un DataFrame\n",
    "    df = ReadCorpus(path)\n",
    "\n",
    "    # Preprocesamiento\n",
    "    df_pre = Preprocess(df)\n",
    "\n",
    "    # Lectura y split de los datos\n",
    "    X = df_pre['title'].values.tolist()\n",
    "    y = df_pre['label'].values.tolist()\n",
    "\n",
    "    X = count_vect.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "    model = TrainModel(X_train, y_train)\n",
    "\n",
    "    # Impresión de métricas\n",
    "    Predicciones = model.predict(X_test)\n",
    "    ValidateModel(y_test, Predicciones)\n",
    "\n",
    "\n",
    "# Prueba para calificación del examen\n",
    "input_file = 'DataSetClickBait.csv'\n",
    "Pipeline(input_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_up",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
