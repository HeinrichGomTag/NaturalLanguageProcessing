{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plantilla de desarrollo para primer examen parcial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pautas:**\n",
    "- La presente plantilla es un EJEMPLO de cómo ordenar el código de tu examen\n",
    "- Tienes la libertad DE AGREGAR todos los métodos y secciones en el examen que consideres necesarias\n",
    "- Realizar el desarrollo por medio de métodos, por ejemplo, ReadInfo(), TrainModel(), etc \n",
    "- Los métodos deberán de estar lo mas claro y modularizados que sea posible\n",
    "- Realizar la documentación de cada método por medio de comentarios y DocStrings \n",
    "- Deberás de utilizar un modelo de ML o algún ensamble de os mismos (SVC, DT, NB, KNN, etc)\n",
    "- Recuerda que puedes usar un split de los datos para entrenamiento y validación\n",
    "- Puedes revisar la documentación de Sklearn, o la librría que decidas utilizar para entender los parámetros de entrenamiento de los modelos\n",
    "- NO está permitido el uso de modelos de Deep Learning (DNN, CNN, LSTM, etc.) NI el uso de embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/eubgo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/eubgo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/eubgo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Librerías profe\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Librerías actuales\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí cargamos la información del DataSet de entrenamiento\n",
    "def read_corpus(path):\n",
    "    \"\"\"Este método lee los de datos del corpus y los pasa a un dataFrame\n",
    "\n",
    "    Args:\n",
    "        path (string): Ubicación del archivo de entrada (Corpus)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df.index = np.arange(1, len(df) + 1)\n",
    "    print(\"Elementos en el DataSet:\", len(df))\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(ngram_range=(1, 2))\n",
    "# Longitud: numero de palabras en texto\n",
    "# Diversidad léxica: palabras únicas vs palabras totales en el texto\n",
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text) if len(text) > 0 else 0.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_synonyms(word):\n",
    "    \"\"\"Obtiene sinónimos de una palabra usando WordNet.\n",
    "\n",
    "    Args:\n",
    "        word (str): Palabra para la cual obtener sinónimos.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de sinónimos.\n",
    "    \"\"\"\n",
    "    from nltk.corpus import wordnet\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "def replace_with_synonym(sentence):\n",
    "    \"\"\"Reemplaza palabras en una oración con un sinónimo aleatorio.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): Oración a modificar.\n",
    "\n",
    "    Returns:\n",
    "        str: Oración modificada.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    for i, word in enumerate(words):\n",
    "        synonyms = get_synonyms(word)\n",
    "        if synonyms:\n",
    "            words[i] = random.choice(synonyms).replace(\"_\", \" \")\n",
    "    return ' '.join(words)\n",
    "\n",
    "def augment(df):\n",
    "    df_augmented = df.copy()\n",
    "    clickbait_rows = df_augmented[df_augmented[\"label\"] == \"clickbait\"]\n",
    "    clickbait_rows[\"title\"] = clickbait_rows[\"title\"].apply(replace_with_synonym)\n",
    "    df_combined = pd.concat([df, clickbait_rows], ignore_index=True)\n",
    "    return df_combined\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataReduction(df):\n",
    "    \"\"\"Método para reducir el número de datos en el dataFrame e igualar news y clickbait\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): Dataframe a reducir\n",
    "\n",
    "    Returns:\n",
    "        df: Dataframe reducido\n",
    "    \"\"\"\n",
    "    # Recalcular las variables news y clickbait después de leer el nuevo DataFrame\n",
    "    news = df[df[\"label\"] == \"news\"]\n",
    "    clickbait = df[df[\"label\"] == \"clickbait\"]\n",
    "\n",
    "    # Reducir news para igualar el número de news y clickbait\n",
    "    news = news.sample(len(clickbait))\n",
    "    df = pd.concat([news, clickbait], ignore_index=True)\n",
    "\n",
    "    # # Contar número de news y clickbait\n",
    "    # news = df[df[\"label\"] == \"news\"]\n",
    "    # clickbait = df[df[\"label\"] == \"clickbait\"]\n",
    "    # print(\"news:\", len(news))\n",
    "    # print(\"clickbait:\", len(clickbait))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    \"\"\"Método para preprocesar el texto y agregar características adicionales\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): Dataframe a aplicar transformaciones\n",
    "\n",
    "    Returns:\n",
    "        dataframe: Dataframe transformado con características adicionales\n",
    "    \"\"\"\n",
    "    \n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].str.lower()\n",
    "\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    df[\"title\"] = df[\"title\"].apply(lambda x: \" \".join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "    df[\"title\"] = df[\"title\"].str.replace(\"[^\\w\\s]\", \"\")\n",
    "\n",
    "    df[\"title\"] = df[\"title\"].apply(nltk.word_tokenize)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    df[\"title\"] = df[\"title\"].apply(lambda x: [lemmatizer.lemmatize(word) for word in x if not word.isdigit()])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(x, y):\n",
    "#     \"\"\"Este método realiza el entrenamiento del modelo (Ejemplo)\n",
    "\n",
    "#     Args:\n",
    "#         x (list): Lista con los textos a transformar\n",
    "#         y (list): Lista con los valores de y (Salida)\n",
    "\n",
    "#     Returns:\n",
    "#         model: Modelo entrenado\n",
    "#     \"\"\"\n",
    "#     # Parámetros para GridSearchCV\n",
    "#     param_grid = {\n",
    "#         'n_estimators': [100, 200, 300],\n",
    "#         'max_depth': [None, 10, 20, 30],\n",
    "#         'min_samples_split': [2, 5, 10]\n",
    "#     }\n",
    "\n",
    "#     grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "#     grid_search.fit(x, y)\n",
    "\n",
    "#     # Entrenando el modelo con los mejores parámetros\n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     best_model.fit(x, y)\n",
    "#     return best_model\n",
    "\n",
    "def train_model(x, y):\n",
    "    \"\"\"Este método realiza el entrenamiento del modelo\n",
    "\n",
    "    Args:\n",
    "        x (array): Matriz de características (TF-IDF)\n",
    "        y (list): Lista con los valores de y (Salida)\n",
    "\n",
    "    Returns:\n",
    "        model: Modelo entrenado de Regresión Logística\n",
    "    \"\"\"\n",
    "    model = LogisticRegression()\n",
    "    model.fit(x, y)\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(y_test, predicciones):\n",
    "    # Impresión de matriz de confusión\n",
    "    # print(\"Matriz de confusión:\")\n",
    "    # print(confusion_matrix(Y_test, Predicciones))\n",
    "\n",
    "    # Impresión de procentaje de Accuracy del modelo\n",
    "    print(\"\\nAccuracy del modelo: \")\n",
    "    print(metrics.accuracy_score(y_test, predicciones))\n",
    "\n",
    "    # Impresión de las métricas para el modelo\n",
    "    print(\"\\nMétricas de evaluación:\")\n",
    "    print(classification_report(y_test, predicciones))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline de todo el proceso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elementos en el DataSet: 16823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2810700/3128748921.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clickbait_rows[\"title\"] = clickbait_rows[\"title\"].apply(replace_with_synonym)\n",
      "/tmp/ipykernel_2810700/3128748921.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clickbait_rows[\"title\"] = clickbait_rows[\"title\"].apply(replace_with_synonym)\n"
     ]
    }
   ],
   "source": [
    "path = 'DataSet para entrenamiento del modelo.csv'\n",
    "\n",
    "# Carga la información y crea un DataFrame\n",
    "df = read_corpus(path)\n",
    "\n",
    "# Data augmentation (si lo deseas)\n",
    "for i in range(2):\n",
    "    df = augment(df)\n",
    "\n",
    "# Preprocesamiento\n",
    "df_pre = preprocess(df)\n",
    "\n",
    "# División de los datos\n",
    "X_text = df_pre['title'].values.tolist()\n",
    "X_text = [' '.join(doc) for doc in X_text]\n",
    "y = df_pre['label'].values.tolist()\n",
    "\n",
    "# Crea el vectorizador TF-IDF\n",
    "tfidf_vect = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_tfidf = tfidf_vect.fit_transform(X_text)\n",
    "\n",
    "# Añade las características adicionales (longitud del texto y diversidad léxica)\n",
    "df_pre['text_length'] = df_pre['title'].apply(len)\n",
    "df_pre['lexical_diversity'] = df_pre['title'].apply(lexical_diversity)\n",
    "\n",
    "# Divide los datos en conjuntos de entrenamiento y prueba\n",
    "X_train_tfidf, X_test_tfidf, X_train_feats, X_test_feats, y_train, y_test = train_test_split(\n",
    "    X_tfidf, df_pre[['text_length', 'lexical_diversity']].values, y,  train_size=0.1, test_size=0.02, random_state=42)\n",
    "\n",
    "# Combina las características TF-IDF y las características adicionales\n",
    "X_train_combined = np.hstack((X_train_tfidf.toarray(), X_train_feats))\n",
    "X_test_combined = np.hstack((X_test_tfidf.toarray(), X_test_feats))\n",
    "\n",
    "# Entrena el modelo (en este caso, Regresión Logística)\n",
    "model = train_model(X_train_combined, y_train)\n",
    "\n",
    "# Realiza predicciones\n",
    "y_pred = model.predict(X_test_combined)\n",
    "\n",
    "# Calculate and display accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Evalúa el modelo\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardado de modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle para guardar modelos\n",
    "import pickle\n",
    "\n",
    "filename = \"model_KikeMau.pickle\"\n",
    "\n",
    "# Guardar el modelo\n",
    "pickle.dump(model, open(filename, \"wb\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba del modelo (Parte mas importante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pipeline(input_file, model):\n",
    "#     # Cargamos la información y creamos un DataFrame\n",
    "#     df = read_corpus(path)\n",
    "\n",
    "#     # Preprocesamiento\n",
    "#     df_pre = preprocess(df)\n",
    "\n",
    "#     # Lectura y split de los datos\n",
    "#     X = df_pre['title'].values.tolist()\n",
    "#     X = [' '.join(doc) for doc in X]\n",
    "#     y = df_pre['label'].values.tolist()\n",
    "\n",
    "#     X = tfidf_vect.fit_transform(X)\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.05, test_size=0.01, random_state=42)\n",
    "\n",
    "#     X_train_tfidf = tfidf_vect.transform(X_train)\n",
    "#     X_test_tfidf = tfidf_vect.transform(X_test)\n",
    "    \n",
    "#     trained_model = train_model(X_train_tfidf, y_train, model)\n",
    "\n",
    "#     # Impresión de métricas\n",
    "#     Predicciones = model.predict(X_test)\n",
    "#     validate_model(y_test, Predicciones)\n",
    "\n",
    "# model = RandomForestClassifier()\n",
    "\n",
    "# # Prueba para calificación del examen\n",
    "# input_file = 'DataSet para entrenamiento del modelo.csv'\n",
    "# pipeline(input_file, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_up",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
