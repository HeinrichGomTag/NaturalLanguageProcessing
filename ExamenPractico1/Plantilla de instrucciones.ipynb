{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plantilla de desarrollo para primer examen parcial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pautas:**\n",
    "- La presente plantilla es un EJEMPLO de cómo ordenar el código de tu examen\n",
    "- Tienes la libertad DE AGREGAR todos los métodos y secciones en el examen que consideres necesarias\n",
    "- Realizar el desarrollo por medio de métodos, por ejemplo, ReadInfo(), TrainModel(), etc \n",
    "- Los métodos deberán de estar lo mas claro y modularizados que sea posible\n",
    "- Realizar la documentación de cada método por medio de comentarios y DocStrings \n",
    "- Deberás de utilizar un modelo de ML o algún ensamble de os mismos (SVC, DT, NB, KNN, etc)\n",
    "- Recuerda que puedes usar un split de los datos para entrenamiento y validación\n",
    "- Puedes revisar la documentación de Sklearn, o la librría que decidas utilizar para entender los parámetros de entrenamiento de los modelos\n",
    "- NO está permitido el uso de modelos de Deep Learning (DNN, CNN, LSTM, etc.) NI el uso de embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/mascenci/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mascenci/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Librerías a utilizar\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "import random\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Descargar recursos de nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    Load a dataset from a CSV file, remove rows with missing values, and filter out rows with empty titles.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned dataframe with rows containing valid titles and no missing values.\n",
    "    \n",
    "    Note:\n",
    "        The CSV file 'DataSet para entrenamiento del modelo.csv' should be present in the current working directory.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('DataSet para entrenamiento del modelo.csv').dropna()\n",
    "    df = df[df['title'] != \"\"]\n",
    "    print(\"Shape of the original dataset:\", df.shape)\n",
    "    return df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Perform feature engineering on the given dataframe based on the 'title' column.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input dataframe with a 'title' column.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe with new features added.\n",
    "    \n",
    "    Features added:\n",
    "        - length: Length of the title.\n",
    "        - unique_words: Number of unique words in the title.\n",
    "        - numbers_count: Number of digits in the title.\n",
    "        - exclamation_count: Number of exclamation marks in the title.\n",
    "        - sentiment: Sentiment polarity of the title using TextBlob.\n",
    "        - keyword_[keyword]: Count of specific keywords in the title.\n",
    "        \n",
    "    Note:\n",
    "        Requires TextBlob library for sentiment analysis.\n",
    "    \"\"\"\n",
    "    df['length'] = df['title'].apply(len)\n",
    "    df['unique_words'] = df['title'].apply(lambda x: len(set(x.split())))\n",
    "    df['numbers_count'] = df['title'].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "    df['exclamation_count'] = df['title'].apply(lambda x: x.count('!'))\n",
    "    df['sentiment'] = df['title'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    keywords = [\"top\", \"best\", \"first\", \"most\", \"amazing\", \"incredible\"]\n",
    "    for keyword in keywords:\n",
    "        df[f'keyword_{keyword}'] = df['title'].apply(lambda x: x.split().count(keyword))\n",
    "    return df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    \"\"\"\n",
    "    Retrieve synonyms for a given word using the NLTK WordNet corpus.\n",
    "    \n",
    "    Parameters:\n",
    "        word (str): The input word for which synonyms are to be retrieved.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of synonyms for the given word.\n",
    "    \n",
    "    Note:\n",
    "        Requires NLTK library and WordNet corpus.\n",
    "    \"\"\"\n",
    "    from nltk.corpus import wordnet\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "def replace_with_synonym(sentence):\n",
    "    \"\"\"\n",
    "    Replace words in a sentence with their synonyms.\n",
    "    \n",
    "    Parameters:\n",
    "        sentence (str): The input sentence in which words are to be replaced with synonyms.\n",
    "    \n",
    "    Returns:\n",
    "        str: The sentence with words replaced by their synonyms.\n",
    "    \n",
    "    Note:\n",
    "        Not all words in the sentence may have synonyms or may be replaced.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    for i, word in enumerate(words):\n",
    "        synonyms = get_synonyms(word)\n",
    "        if synonyms:\n",
    "            words[i] = random.choice(synonyms).replace(\"_\", \" \")\n",
    "    return ' '.join(words)\n",
    "\n",
    "def augment(df):\n",
    "    \"\"\"\n",
    "    Augment the dataframe by replacing titles labeled as \"clickbait\" with their synonym-replaced versions.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input dataframe with a 'title' column and a 'label' column.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The augmented dataframe with original and synonym-replaced titles.\n",
    "    \n",
    "    Note:\n",
    "        Only titles with label \"clickbait\" are augmented.\n",
    "    \"\"\"\n",
    "    df_augmented = df.copy()\n",
    "    clickbait_rows = df_augmented[df_augmented[\"label\"] == \"clickbait\"].copy()\n",
    "    clickbait_rows[\"title\"] = clickbait_rows[\"title\"].apply(replace_with_synonym)\n",
    "    return pd.concat([df, clickbait_rows], ignore_index=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the input text by performing the following steps:\n",
    "    1. Convert to lowercase.\n",
    "    2. Remove non-alphabetic characters.\n",
    "    3. Remove English stopwords.\n",
    "    4. Lemmatize the words using spaCy.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): The input text to be preprocessed.\n",
    "    \n",
    "    Returns:\n",
    "        str: The preprocessed text.\n",
    "    \n",
    "    Note:\n",
    "        Requires NLTK library for stopwords and spaCy for lemmatization.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = ''.join([char for char in text if char.isalpha() or char.isspace()])\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(X_train, Y_train, X_test, Y_test, X, Y):\n",
    "    \"\"\"\n",
    "    Train and evaluate machine learning models using pipelines and grid search.\n",
    "    \n",
    "    The function trains two models: Logistic Regression and LinearSVC. It uses a pipeline to preprocess the data \n",
    "    with TfidfVectorizer for the 'title' column and StandardScaler for numeric features. Hyperparameters are optimized \n",
    "    using GridSearchCV, and the models are evaluated on the test set and using cross-validation.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (pd.DataFrame): Training data features.\n",
    "        Y_train (pd.Series): Training data labels.\n",
    "        X_test (pd.DataFrame): Test data features.\n",
    "        Y_test (pd.Series): Test data labels.\n",
    "        X (pd.DataFrame): Complete data features for cross-validation.\n",
    "        Y (pd.Series): Complete data labels for cross-validation.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with model names as keys and their best estimators as values.\n",
    "    \n",
    "    Note:\n",
    "        Requires NLTK for stopwords, scikit-learn for modeling and evaluation, and spaCy for text processing.\n",
    "    \"\"\"\n",
    "    # Create a transformer that applies TfidfVectorizer to the 'title' column and StandardScaler to the numeric features.\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('text', TfidfVectorizer(stop_words=nltk.corpus.stopwords.words('english')), 'title'),\n",
    "            ('num', StandardScaler(), [col for col in X.columns if col != 'title'])\n",
    "        ])\n",
    "\n",
    "    # Define pipelines for each model\n",
    "    pipelines = {\n",
    "        'Logistic Regression': Pipeline([('preprocessor', preprocessor),\n",
    "                                        ('clf', LogisticRegression(max_iter=1000))]),\n",
    "        'LinearSVC': Pipeline([('preprocessor', preprocessor),\n",
    "                                ('clf', LinearSVC(dual=False))])\n",
    "    }\n",
    "\n",
    "    # Define parameters for grid search\n",
    "    param_grid = {\n",
    "        'preprocessor__text__max_df': [0.85, 0.9, 0.95],\n",
    "        'preprocessor__text__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "        'clf__C': [0.1, 1, 10]\n",
    "    }\n",
    "\n",
    "    # Optimize hyperparameters and evaluate each model\n",
    "    best_estimators = {}\n",
    "    for name, pipeline in pipelines.items():\n",
    "        grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "        grid_search.fit(X_train, Y_train)\n",
    "        best_estimators[name] = grid_search.best_estimator_\n",
    "\n",
    "        y_pred = best_estimators[name].predict(X_test)\n",
    "        accuracy = accuracy_score(Y_test, y_pred)\n",
    "        print(f\"Accuracy of the optimized {name} model: {accuracy:.4f}\")\n",
    "\n",
    "        cv_accuracy = cross_val_score(best_estimators[name], X, Y, cv=5, scoring='accuracy').mean()\n",
    "        print(f\"Average cross-validation accuracy for {name}: {cv_accuracy:.4f}\\n\")\n",
    "\n",
    "    return best_estimators\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(best_estimators, X_test, Y_test, X_train, Y_train, X, Y):\n",
    "    \"\"\"\n",
    "    Validate the performance of an ensemble model created using the best estimators.\n",
    "    \n",
    "    The function creates an ensemble model using the VotingClassifier with the best estimators provided. \n",
    "    It then fits the ensemble model on the training data and evaluates its performance on the test set \n",
    "    and using cross-validation.\n",
    "    \n",
    "    Parameters:\n",
    "        best_estimators (dict): A dictionary with model names as keys and their best estimators as values.\n",
    "        X_test (pd.DataFrame): Test data features.\n",
    "        Y_test (pd.Series): Test data labels.\n",
    "        X_train (pd.DataFrame): Training data features.\n",
    "        Y_train (pd.Series): Training data labels.\n",
    "        X (pd.DataFrame): Complete data features for cross-validation.\n",
    "        Y (pd.Series): Complete data labels for cross-validation.\n",
    "    \n",
    "    Returns:\n",
    "        VotingClassifier: The trained ensemble model.\n",
    "    \n",
    "    Note:\n",
    "        Requires scikit-learn for modeling and evaluation.\n",
    "    \"\"\"\n",
    "    ensemble_model = VotingClassifier(estimators=[\n",
    "        ('Logistic Regression', best_estimators['Logistic Regression']),\n",
    "        ('LinearSVC', best_estimators['LinearSVC'])\n",
    "    ], voting='hard')\n",
    "\n",
    "    ensemble_model.fit(X_train, Y_train)\n",
    "    ensemble_predictions = ensemble_model.predict(X_test)\n",
    "    ensemble_accuracy = accuracy_score(Y_test, ensemble_predictions)\n",
    "    print(\"\\nEnsemble model accuracy:\", ensemble_accuracy)\n",
    "\n",
    "    ensemble_cv_accuracy = cross_val_score(ensemble_model, X, Y, cv=5, scoring='accuracy').mean()\n",
    "    print(f\"Average cross-validation accuracy for the ensemble model: {ensemble_cv_accuracy:.4f}\\n\")\n",
    "\n",
    "    return ensemble_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardado del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_model(model, filename=\"model_KikeMau.pickle\"):\n",
    "    \"\"\"\n",
    "    Save the trained machine learning model to a file using pickle.\n",
    "    \n",
    "    Parameters:\n",
    "        model (object): The trained machine learning model to be saved.\n",
    "        filename (str, optional): The name of the file where the model will be saved. \n",
    "                                  Defaults to \"model_KikeMau.pickle\".\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \n",
    "    Note:\n",
    "        The function will print a message indicating the location where the model was saved.\n",
    "    \"\"\"\n",
    "    # Save the model\n",
    "    pickle.dump(model, open(filename, \"wb\"))\n",
    "    print(f\"Model saved in {filename}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline de todo el proceso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the original dataset: (16823, 2)\n",
      "Shape of the dataset after Data Augmentation: (27101, 2)\n",
      "Shape of the dataset after Preprocessing: (27101, 2)\n",
      "Shape of the dataset after Feature Engineering: (27101, 13)\n",
      "X Dimensions: (27101, 12)\n",
      "Y Dimensions: (27101,)\n",
      "X_train Dimensions: (21680, 12)\n",
      "Y_train Dimensions: (21680,)\n",
      "Accuracy of the optimized Logistic Regression model: 0.8585\n",
      "Average cross-validation accuracy for Logistic Regression: 0.8470\n",
      "\n",
      "Accuracy of the optimized LinearSVC model: 0.8659\n",
      "Average cross-validation accuracy for LinearSVC: 0.8516\n",
      "\n",
      "\n",
      "Ensemble model accuracy: 0.8625714812765173\n",
      "Average cross-validation accuracy for the ensemble model: 0.8492\n",
      "\n",
      "\n",
      "Ensemble model accuracy: 0.8625714812765173\n",
      "Average cross-validation accuracy for the ensemble model: 0.8492\n",
      "\n",
      "Model saved in model_KikeMau.pickle\n"
     ]
    }
   ],
   "source": [
    "def main_pipeline():\n",
    "    \"\"\"\n",
    "    Execute the main pipeline that encompasses the entire process of loading the dataset, data augmentation, \n",
    "    preprocessing, feature engineering, model training, validation, and saving the trained model.\n",
    "    \n",
    "    The pipeline performs the following steps:\n",
    "    1. Load the dataset.\n",
    "    2. Check for class imbalance and augment data if necessary.\n",
    "    3. Apply text preprocessing.\n",
    "    4. Apply feature engineering.\n",
    "    5. Split the data into training and test sets.\n",
    "    6. Train machine learning models and validate their performance.\n",
    "    7. Train an ensemble model and validate its performance.\n",
    "    8. Save the trained ensemble model to a file.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \n",
    "    Note:\n",
    "        The function will print various messages indicating the progress and results at each step.\n",
    "    \"\"\"\n",
    "    df = load_dataset()\n",
    "\n",
    "    # Check imbalance and augment data if necessary\n",
    "    label_counts = df['label'].value_counts()\n",
    "    majority_count = label_counts.max()\n",
    "    minority_count = label_counts.min()\n",
    "\n",
    "    while minority_count / majority_count < 0.7:\n",
    "        df = augment(df)\n",
    "        # Recalculate counts after augmentation\n",
    "        label_counts = df['label'].value_counts()\n",
    "        majority_count = label_counts.max()\n",
    "        minority_count = label_counts.min()\n",
    "\n",
    "    print(\"Shape of the dataset after Data Augmentation:\", df.shape)\n",
    "\n",
    "    # Apply Preprocessing\n",
    "    df['title'] = df['title'].apply(preprocess_text)\n",
    "    print(\"Shape of the dataset after Preprocessing:\", df.shape)\n",
    "\n",
    "    # Apply Feature Engineering\n",
    "    df = feature_engineering(df)\n",
    "    print(\"Shape of the dataset after Feature Engineering:\", df.shape)\n",
    "\n",
    "    Y = df['label']\n",
    "    X = df.drop('label', axis=1)\n",
    "\n",
    "    print(\"X Dimensions:\", X.shape)\n",
    "    print(\"Y Dimensions:\", Y.shape)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "    print(\"X_train Dimensions:\", X_train.shape)\n",
    "    print(\"Y_train Dimensions:\", Y_train.shape)\n",
    "\n",
    "    best_estimators = train_models(X_train, Y_train, X_test, Y_test, X, Y)\n",
    "    validate_model(best_estimators, X_test, Y_test, X_train, Y_train, X, Y)\n",
    "\n",
    "    ensemble_model = validate_model(best_estimators, X_test, Y_test, X_train, Y_train, X, Y)  # Capture the returned model\n",
    "\n",
    "    # Save the ensemble model\n",
    "    save_model(ensemble_model)\n",
    "\n",
    "\n",
    "# Ejecutar el pipeline completo\n",
    "main_pipeline()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba del modelo (Parte mas importante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the loaded model: 0.9583\n"
     ]
    }
   ],
   "source": [
    "def test_model(model_filename=\"model_KikeMau.pickle\", csv_filename=\"DataSet para entrenamiento del modelo.csv\"):\n",
    "    \"\"\"\n",
    "    Load a trained machine learning model from a pickle file and test its performance using data from a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "        model_filename (str, optional): The name of the pickle file where the model is saved. \n",
    "                                         Defaults to \"model_KikeMau.pickle\".\n",
    "        csv_filename (str, optional): The name of the CSV file containing the test data. \n",
    "                                      Defaults to \"DataSet para entrenamiento del modelo.csv\".\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \n",
    "    Note:\n",
    "        The function will print the accuracy of the loaded model on the test data.\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    loaded_model = pickle.load(open(model_filename, \"rb\"))\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(csv_filename)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    df['title'] = df['title'].apply(preprocess_text)\n",
    "    df = feature_engineering(df)\n",
    "    \n",
    "    Y_test = df['label']\n",
    "    X_test = df.drop('label', axis=1)\n",
    "    \n",
    "    # Predict using the loaded model\n",
    "    y_pred = loaded_model.predict(X_test)\n",
    "    accuracy = accuracy_score(Y_test, y_pred)\n",
    "    print(f\"Accuracy of the loaded model: {accuracy:.4f}\")\n",
    "\n",
    "test_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_up",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
