{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plantilla de desarrollo para primer examen parcial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pautas:**\n",
    "- La presente plantilla es un EJEMPLO de cómo ordenar el código de tu examen\n",
    "- Tienes la libertad DE AGREGAR todos los métodos y secciones en el examen que consideres necesarias\n",
    "- Realizar el desarrollo por medio de métodos, por ejemplo, ReadInfo(), TrainModel(), etc \n",
    "- Los métodos deberán de estar lo mas claro y modularizados que sea posible\n",
    "- Realizar la documentación de cada método por medio de comentarios y DocStrings \n",
    "- Deberás de utilizar un modelo de ML o algún ensamble de os mismos (SVC, DT, NB, KNN, etc)\n",
    "- Recuerda que puedes usar un split de los datos para entrenamiento y validación\n",
    "- Puedes revisar la documentación de Sklearn, o la librría que decidas utilizar para entender los parámetros de entrenamiento de los modelos\n",
    "- NO está permitido el uso de modelos de Deep Learning (DNN, CNN, LSTM, etc.) NI el uso de embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/eubgo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/eubgo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/eubgo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Librerías profe\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Librerías actuales\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí cargamos la información del DataSet de entrenamiento\n",
    "def read_corpus(path):\n",
    "    \"\"\"Este método lee los de datos del corpus y los pasa a un dataFrame\n",
    "\n",
    "    Args:\n",
    "        path (string): Ubicación del archivo de entrada (Corpus)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df.index = np.arange(1, len(df) + 1)\n",
    "    print(\"Elementos en el DataSet:\", len(df))\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(ngram_range=(1, 2))\n",
    "# Longitud: numero de palabras en texto\n",
    "# Diversidad léxica: palabras únicas vs palabras totales en el texto"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserta lo que consideres necesario aquí, por ejemplo\n",
    "def preprocess(df):\n",
    "    \"\"\"Método para preprocesar el texto\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): Dataframe a aplicar transformaciones\n",
    "\n",
    "    Returns:\n",
    "        dataframe: Dataframe transformado\n",
    "    \"\"\"\n",
    "    \n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].str.lower()\n",
    "\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    df[\"title\"] = df[\"title\"].apply(lambda x: \" \".join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "    df[\"title\"] = df[\"title\"].str.replace(\"[^\\w\\s]\", \"\")\n",
    "\n",
    "    df[\"title\"] = df[\"title\"].apply(nltk.word_tokenize)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    df[\"title\"] = df[\"title\"].apply(lambda x: [lemmatizer.lemmatize(word) for word in x if not word.isdigit()])\n",
    "\n",
    "    # df['text_length'] = df['title'].apply(len)  # Longitud del texto\n",
    "    # df['lexical_diversity'] = df['title'].apply(lambda x: len(set(x)) / len(x))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/eubgo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elementos en el DataSet: 16823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_71873/4217138993.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clickbait_rows[\"title\"] = clickbait_rows[\"title\"].apply(replace_with_synonym)\n",
      "/tmp/ipykernel_71873/4217138993.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clickbait_rows[\"title\"] = clickbait_rows[\"title\"].apply(replace_with_synonym)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate rows:            label                                              title\n",
      "540         news            The Situation RoomVerifizierter Account\n",
      "1127        news            The Situation RoomVerifizierter Account\n",
      "1674        news                                           Facebook\n",
      "1783        news  Big Mac Makeover Helps McDonald's Overcome Res...\n",
      "2117   clickbait  17 Limited-Edition Beauty Products You Need To...\n",
      "...          ...                                                ...\n",
      "26903  clickbait  ten subject you claim for to passion ahead the...\n",
      "26907  clickbait  link up Hoosier State breast feeding inside se...\n",
      "26911  clickbait           literally exactly XIX really gravid barf\n",
      "27081  clickbait  ten subject you claim for to passion ahead the...\n",
      "27088  clickbait  sextuplet low-sweat recitation to dumbfound th...\n",
      "\n",
      "[3652 rows x 2 columns]\n",
      "           label                                              title\n",
      "27096  clickbait  [brokaw, :, ‘, democrat, got, dumbfound, antio...\n",
      "27097  clickbait  [pay, polish, institute, in, :, comey, kudos, ...\n",
      "27098  clickbait  [native, american, language, dr., amplificatio...\n",
      "27099  clickbait  [psychologist, got, dumbfound, separate, ten, ...\n",
      "27100  clickbait  [smartphone, passion, antiophthalmic, factor, ...\n",
      "news: 13397\n",
      "clickbait: 13704\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "def get_synonyms(word):\n",
    "    \"\"\"Obtiene sinónimos de una palabra usando WordNet.\n",
    "\n",
    "    Args:\n",
    "        word (str): Palabra para la cual obtener sinónimos.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de sinónimos.\n",
    "    \"\"\"\n",
    "    from nltk.corpus import wordnet\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "def replace_with_synonym(sentence):\n",
    "    \"\"\"Reemplaza palabras en una oración con un sinónimo aleatorio.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): Oración a modificar.\n",
    "\n",
    "    Returns:\n",
    "        str: Oración modificada.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    for i, word in enumerate(words):\n",
    "        synonyms = get_synonyms(word)\n",
    "        if synonyms:\n",
    "            words[i] = synonyms[0].replace(\"_\", \" \")\n",
    "    return ' '.join(words)\n",
    "\n",
    "def augment(df):\n",
    "    df_augmented = df.copy()\n",
    "    clickbait_rows = df_augmented[df_augmented[\"label\"] == \"clickbait\"]\n",
    "    clickbait_rows[\"title\"] = clickbait_rows[\"title\"].apply(replace_with_synonym)\n",
    "    df_combined = pd.concat([df, clickbait_rows], ignore_index=True)\n",
    "    return df_combined\n",
    "\n",
    "\n",
    "\n",
    "#probar augment\n",
    "df = ReadCorpus(\"DataSet para entrenamiento del modelo.csv\")\n",
    "df = augment(df)\n",
    "df = augment(df)\n",
    "\n",
    "#count duplicate rows\n",
    "duplicate_rows = df[df.duplicated()]\n",
    "print(\"duplicate rows:\", duplicate_rows)\n",
    "\n",
    "df = Preprocess(df)\n",
    "\n",
    "print(df.tail())\n",
    "\n",
    "# contar numero de news y clickbait\n",
    "news = df[df[\"label\"] == \"news\"]\n",
    "clickbait = df[df[\"label\"] == \"clickbait\"]\n",
    "print(\"news:\", len(news))\n",
    "print(\"clickbait:\", len(clickbait))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "\n",
    "\n",
    "def train_model(x, y):\n",
    "    \"\"\"Este método realiza el entrenamiento del modelo (Ejemplo)\n",
    "\n",
    "    Args:\n",
    "        x (list): Lista con los textos a transformar\n",
    "        y (list): Lista con los valores de y (Salida)\n",
    "\n",
    "    Returns:\n",
    "        model: Modelo entrenado\n",
    "    \"\"\"\n",
    "    # Parámetros para GridSearchCV\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(x, y)\n",
    "\n",
    "    # Entrenando el modelo con los mejores parámetros\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_model.fit(x, y)\n",
    "    return best_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(y_test, predicciones):\n",
    "    # Impresión de matriz de confusión\n",
    "    # print(\"Matriz de confusión:\")\n",
    "    # print(confusion_matrix(Y_test, Predicciones))\n",
    "\n",
    "    # Impresión de procentaje de Accuracy del modelo\n",
    "    print(\"\\nAccuracy del modelo: \")\n",
    "    print(metrics.accuracy_score(y_test, predicciones))\n",
    "\n",
    "    # Impresión de las métricas para el modelo\n",
    "    print(\"\\nMétricas de evaluación:\")\n",
    "    print(classification_report(y_test, predicciones))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline de todo el proceso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elementos en el DataSet: 16823\n"
     ]
    }
   ],
   "source": [
    "# Cargamos la información y creamos un DataFrame\n",
    "path = 'DataSet para entrenamiento del modelo.csv'\n",
    "df = read_corpus(path)\n",
    "\n",
    "# Preprocesamiento\n",
    "df_pre = preprocess(df)\n",
    "\n",
    "# Lectura y split de los datos\n",
    "X = df_pre['title'].values.tolist()\n",
    "X = [' '.join(tokens) for tokens in X]\n",
    "X = tfidf_vect.fit_transform(X)\n",
    "# X = np.hstack((X.toarray(), df_pre[['text_length', 'lexical_diversity']].values))\n",
    "\n",
    "y = df_pre['label'].values.tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "model = train_model(X_train, y_train)\n",
    "\n",
    "# Impresión de métricas\n",
    "Predicciones = model.predict(X_test)\n",
    "validate_model(y_test, Predicciones)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardado de modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle para guardar modelos\n",
    "import pickle\n",
    "\n",
    "filename = \"model_KikeMau.pickle\"\n",
    "\n",
    "# Guardar el modelo\n",
    "pickle.dump(model, open(filename, \"wb\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba del modelo (Parte mas importante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Pipeline() missing 1 required positional argument: 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m# Prueba para calificación del examen\u001b[39;00m\n\u001b[1;32m     23\u001b[0m input_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mDataSetClickBait.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 24\u001b[0m Pipeline(input_file)\n",
      "\u001b[0;31mTypeError\u001b[0m: Pipeline() missing 1 required positional argument: 'model'"
     ]
    }
   ],
   "source": [
    "def pipeline(input_file, model):\n",
    "    # Cargamos la información y creamos un DataFrame\n",
    "    df = read_corpus(path)\n",
    "\n",
    "    # Preprocesamiento\n",
    "    df_pre = preprocess(df)\n",
    "\n",
    "    # Lectura y split de los datos\n",
    "    X = df_pre['title'].values.tolist()\n",
    "    y = df_pre['label'].values.tolist()\n",
    "\n",
    "    X = tfidf_vect.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "    model = train_model(X_train, y_train)\n",
    "\n",
    "    # Impresión de métricas\n",
    "    Predicciones = model.predict(X_test)\n",
    "    validate_model(y_test, Predicciones)\n",
    "\n",
    "\n",
    "# Prueba para calificación del examen\n",
    "input_file = 'DataSet para entrenamiento del modelo.csv'\n",
    "pipeline(input_file, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_up",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
