{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N921vKRx4RIH"
   },
   "source": [
    "# Tarea 02: idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nd_x9Fv4rMre"
   },
   "source": [
    "### En esta tarea, se trabajará con un corpus de textos para obtener las palabras mas y menos relevantes a partir de un análisis por tf-idf. Deberás seguir los pasos de este notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPaso 1) \\nDel archivo adjunto (Questions.csv), extraer las primeras 2,000 preguntas presentes en la columna \"Title\", \\ny generar un nuevo archivo que SOLO contenga dichas preguntas.\\nNOTA: El archivo es muy grande, por lo que deberás de procesarlo en tu equipo local (No usar colab), y \\ngenerar con ayuda de un script el nuevo archivo, si llegas a tener problemas con el tipo de codificación\\ny se generan caracteres raros, deberás resolverlo con la codificación adecuada de lectura\\n\\nPaso 2) \\nTraducirlas las preguntas del nuevo archivo al idioma Español (Investiga cómo se realiza el proceso de \\ntraducción automático utilizando Google Sheets y Google Translate)\\nSe deberá de agregar una nueva columna al archivo .csv llamada \"Textos_traducidos\" donde se incluirán las \\ntraducciones de los textos originales (Incluir este nuevo .csv en la entrega de tu tarea)\\n\\nPaso 3) \\nCon este nuevo .csv de 2,000 textos traducidos al español, crear un dataset (de 2,000 textos) y aplicar \\nOBLIGATORIAMENTE los siguientes preprocesamientos:\\n- Lematización de todas las palabras\\n- Filtrado de StopWords\\n- Pasar todo a minúsculas\\n\\nPaso 4) \\nDespués de haber limpiado el dataset anterior, generar el vector de idf correspondiente a TODOS los textos\\ny mostrarlo en pantalla\\n\\nPaso 5) Regresar al Dataset original, y remover todas aquellas palabras que contengan un valor de idf menor\\nal promedio de TODOS los idfs de la tabla obtenida\\n\\nPaso 6)\\nGenerar nuevamente la tabla de idf a partir de valores de idf para los textos filtrados y mostrarla en pantalla\\n\\nPaso 7) Imprimir en pantalla el top de las 10 palabras MAS relevantes, y el top de las 20 palabras MENOS \\nrelevantes\\n\\nNOTA: Recuerda que deberás de entregar 2 archivos, un .csv con los 2,000 textos originales y sus traducciones\\ny un .ipynb con todo el procedimiento realizado con sus respectivos comentarios, y DocStrings\\nIMPORTANTE: Todo el proceso deberá realizarse por medio de métodos, NO se aceptará programación estructurada,\\npor lo que, por ejemplo, deberá haber un método para filtrar StopWords, otro para obtener el promedio de \\nidf de todo el conjunto de palabras, etc. \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7InFgho6rytt"
   },
   "outputs": [],
   "source": [
    "# Recuerda que todos los métodos que utilices deberán de contar con el formato DocString\n",
    "# como en el ejemplo que se muestra a continuación:\n",
    "# Ejemplo de formato Docstrings:\n",
    "\n",
    "# def NombreFuncion(arg1, arg2, arg3):\n",
    "#     \"\"\"\n",
    "#     Este método sirve para... utilizando... y devuelve...\n",
    "    \n",
    "#     Args:\n",
    "#         string arg1: Esta es una cadena de texto que...\n",
    "#         int arg 2: Es un número entero que se usa para...\n",
    "#         dict arg 3: Diccionario que sirve para...\n",
    "\n",
    "#     Returns:\n",
    "#         string: Cadena del texto ya corregido...\n",
    "#         int: El la cantidad de correcciones realizadas...\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Aquí debe de ir la lógica de la función (Después de la documentación)\n",
    "#     Texto = \"\"\n",
    "#     corr = 5\n",
    "    \n",
    "#   return Texto, corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/saramiranda/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/saramiranda/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/saramiranda/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "def extract_and_save_titles(input_file, output_file, num_rows=2000):\n",
    "    \"\"\"\n",
    "    Extract a specified number of rows from the \"Title\" column of a CSV file and save to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        input_file (str): Path to the input CSV file.\n",
    "        output_file (str): Path to save the extracted titles as a new CSV file.\n",
    "        num_rows (int, optional): Number of rows to extract from the \"Title\" column. Default is 2000.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_file, encoding=\"latin1\")  # FIXME encoding can be incorrect\n",
    "    df_titles = pd.DataFrame(df.head(num_rows)[\"Title\"])\n",
    "    df_titles.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "class TextAnalysis:\n",
    "    \"\"\"\n",
    "    Class to analyze text data.\n",
    "\n",
    "    Parameters:\n",
    "       filepath (str): Path to the input CSV file.\n",
    "    \"\"\"\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = nltk.corpus.stopwords.words(\"spanish\")\n",
    "        self.vectorizer = TfidfVectorizer(use_idf=True)\n",
    "        self.read_data()\n",
    "        self.preprocess_translated_text()\n",
    "        self.calculate_idf()\n",
    "        self.filter_below_average_idf()\n",
    "\n",
    "    \"\"\"\n",
    "    Method to read the data from the input CSV file.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the input CSV file.\n",
    "    Returns:\n",
    "        Nothing, it just creates a dataframe with the translated texts.\n",
    "    \"\"\"\n",
    "    def read_data(self):\n",
    "        self.df = pd.read_csv(self.filepath)[\"Translated_Texts\"]\n",
    "\n",
    "    \"\"\"\n",
    "    Method to preprocess the translated text.\n",
    "    By using the nlkt library, the text is tokenized, lemmatized, and stop words are removed.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text to be preprocessed.\n",
    "    Returns:\n",
    "        str: Preprocessed text.\n",
    "    \"\"\"\n",
    "    def preprocess_translated_text(self):\n",
    "        def process(text):\n",
    "            words = nltk.word_tokenize(text)\n",
    "            words = [\n",
    "                self.lemmatizer.lemmatize(word.lower())\n",
    "                for word in words\n",
    "                if word.lower() not in self.stop_words\n",
    "            ]\n",
    "            return \" \".join(words)\n",
    "        self.df = self.df.apply(process)\n",
    "\n",
    "    \"\"\"\n",
    "    Method to calculate the IDF values for the text.\n",
    "\n",
    "    Args:\n",
    "        tfidf_matrix (array): Array with the TF-IDF values for the text.\n",
    "        features (list): List of the features of the text.\n",
    "        idf (list): List of the IDF values for the text.\n",
    "        average_idf (float): Average IDF value for the text.\n",
    "        idf_df (dataframe): Dataframe with the IDF values for the text.\n",
    "    Returns:\n",
    "        Nothing, it just calculates\n",
    "    \"\"\"\n",
    "    def calculate_idf(self):\n",
    "        tfidf_matrix = self.vectorizer.fit_transform(self.df)\n",
    "        features = self.vectorizer.get_feature_names_out()\n",
    "        idf = self.vectorizer.idf_\n",
    "        self.idf_dict = dict(zip(features, idf))\n",
    "        self.average_idf = np.mean(idf)\n",
    "        self.idf_df = pd.DataFrame(\n",
    "            list(self.idf_dict.items()), columns=[\"Word\", \"IDF Value\"]\n",
    "        )\n",
    "    \"\"\"\n",
    "    Method to filter the text below the average IDF value.\n",
    "\n",
    "    Args:\n",
    "        below_average_idf (dataframe): Dataframe with the filtered IDF values for the text.\n",
    "    Returns:   \n",
    "        Nothing, it just filters the text below the average IDF value.\n",
    "    \"\"\"\n",
    "    def filter_below_average_idf(self):\n",
    "        def filter_below_average_idf(text):\n",
    "            words = text.split()\n",
    "            return \" \".join(\n",
    "                [\n",
    "                    word\n",
    "                    for word in words\n",
    "                    if self.idf_dict.get(word, 0) >= self.average_idf\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.below_average_idf = self.df.apply(filter_below_average_idf)\n",
    "    \"\"\"\n",
    "    Method to calculate the IDF values for the filtered idf's, and creates a dataframe which\n",
    "    is saved as a CSV file and returned.\n",
    "\n",
    "    Args:\n",
    "        filtered_tfidf_matrix (array): Array with the TF-IDF values for the filtered text.\n",
    "        features (list): List of the features of the filtered text.\n",
    "        idf (list): List of the IDF values for the filtered text.\n",
    "\n",
    "    Returns:\n",
    "        Nothing, it just calculates the IDF values for the filtered text.\n",
    "    \"\"\"\n",
    "    def calculate_filtered_idf(self):\n",
    "        filtered_tfidf_matrix = self.vectorizer.fit_transform(self.below_average_idf)\n",
    "        features = self.vectorizer.get_feature_names_out()\n",
    "        idf = self.vectorizer.idf_ \n",
    "        self.idf_dict = dict(zip(features, idf))\n",
    "        self.filtered_df = pd.DataFrame(\n",
    "            list(self.idf_dict.items()), columns=[\"Word\", \"IDF Value\"]\n",
    "        )\n",
    "    \"\"\"\n",
    "    Method to return the top and bottom words of the IDF values which are saved as dataframes\n",
    "    \n",
    "    Args:\n",
    "        top_words (dataframe): Dataframe with the top words of the IDF values.\n",
    "        bottom_words (dataframe): Dataframe with the bottom words of the IDF values.\n",
    "\n",
    "    Returns:    \n",
    "        top_words (dataframe): Dataframe with the top words of the IDF values.\n",
    "        bottom_words (dataframe): Dataframe with the bottom words of the IDF values.\n",
    "    \"\"\"\n",
    "    def top_bottom_words(self, top_n=10, bottom_n=20):\n",
    "        sorted_idf = sorted(self.idf_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_words = sorted_idf[:top_n]\n",
    "        bottom_words = sorted_idf[-bottom_n:]\n",
    "        df_top_words = pd.DataFrame(top_words, columns=[\"Word\", \"IDF Value\"])\n",
    "        df_bottom_words = pd.DataFrame(bottom_words, columns=[\"Word\", \"IDF Value\"])\n",
    "        return df_top_words, df_bottom_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial IDF Table:\n",
      "        Word  IDF Value\n",
      "0       000z   7.908255\n",
      "1         04   7.908255\n",
      "2       0x1a   7.908255\n",
      "3         10   7.215108\n",
      "4        100   7.908255\n",
      "...      ...        ...\n",
      "3254   única   6.991964\n",
      "3255  únicas   7.908255\n",
      "3256   único   7.908255\n",
      "3257  únicos   7.908255\n",
      "3258    útil   7.502790\n",
      "\n",
      "[3259 rows x 2 columns]\n",
      "\n",
      "Filtered IDF Table:\n",
      "        Word  IDF Value\n",
      "0       0x1a   7.908255\n",
      "1        100   7.908255\n",
      "2       1000   7.908255\n",
      "3        104   7.908255\n",
      "4       1123   7.908255\n",
      "...      ...        ...\n",
      "1741  índice   7.908255\n",
      "1742  último   7.908255\n",
      "1743  únicas   7.908255\n",
      "1744   único   7.908255\n",
      "1745  únicos   7.908255\n",
      "\n",
      "[1746 rows x 2 columns]\n",
      "\n",
      "Top 10 Words:\n",
      "   Word  IDF Value\n",
      "0  0x1a   7.908255\n",
      "1   100   7.908255\n",
      "2  1000   7.908255\n",
      "3   104   7.908255\n",
      "4  1123   7.908255\n",
      "5    20   7.908255\n",
      "6  2006   7.908255\n",
      "7  2009   7.908255\n",
      "8    23   7.908255\n",
      "9   301   7.908255\n",
      "\n",
      "Bottom 20 Words:\n",
      "       Word  IDF Value\n",
      "0     xhtml   7.908255\n",
      "1        xl   7.908255\n",
      "2      xlrd   7.908255\n",
      "3    xmodem   7.908255\n",
      "4      xp64   7.908255\n",
      "5      xslt   7.908255\n",
      "6     yahoo   7.908255\n",
      "7   youtube   7.908255\n",
      "8   zemanta   7.908255\n",
      "9     zombi   7.908255\n",
      "10      zsi   7.908255\n",
      "11    álbum   7.908255\n",
      "12   ángulo   7.908255\n",
      "13    áreas   7.908255\n",
      "14    éxito   7.908255\n",
      "15   índice   7.908255\n",
      "16   último   7.908255\n",
      "17   únicas   7.908255\n",
      "18    único   7.908255\n",
      "19   únicos   7.908255\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Paso 1)\n",
    "    Del archivo adjunto (Questions.csv), extraer las primeras 2,000 preguntas presentes en la columna \"Title\",\n",
    "    y generar un nuevo archivo que SOLO contenga dichas preguntas.\n",
    "    NOTA: El archivo es muy grande, por lo que deberás de procesarlo en tu equipo local (No usar colab), y\n",
    "    generar con ayuda de un script el nuevo archivo, si llegas a tener problemas con el tipo de codificación\n",
    "    y se generan caracteres raros, deberás resolverlo con la codificación adecuada de lectura\n",
    "    \"\"\"\n",
    "    extract_and_save_titles(\"Questions.csv\", \"Questions_2000.csv\")\n",
    "\n",
    "    \"\"\"\n",
    "    Paso 2) \n",
    "    Traducirlas las preguntas del nuevo archivo al idioma Español (Investiga cómo se realiza el proceso de \n",
    "    traducción automático utilizando Google Sheets y Google Translate)\n",
    "    Se deberá de agregar una nueva columna al archivo .csv llamada \"Textos_traducidos\" donde se incluirán las \n",
    "    traducciones de los textos originales (Incluir este nuevo .csv en la entrega de tu tarea)\n",
    "    \"\"\"\n",
    "    # In TranslatedQuestions.csv file\n",
    "\n",
    "    \"\"\"\n",
    "    Paso 3) \n",
    "    Con este nuevo .csv de 2,000 textos traducidos al español, crear un dataset (de 2,000 textos) y aplicar \n",
    "    OBLIGATORIAMENTE los siguientes preprocesamientos:\n",
    "    - Lematización de todas las palabras\n",
    "    - Filtrado de StopWords\n",
    "    - Pasar todo a minúsculas\n",
    "    \"\"\"\n",
    "    analysis = TextAnalysis(\"TranslatedQuestions.csv\")\n",
    "\n",
    "    \"\"\"\n",
    "    Paso 4) \n",
    "    Después de haber limpiado el dataset anterior, generar el vector de idf correspondiente a TODOS los textos\n",
    "    y mostrarlo en pantalla\n",
    "    \"\"\"\n",
    "    print(\"Initial IDF Table:\")\n",
    "    analysis.calculate_idf()\n",
    "    print(analysis.idf_df)\n",
    "\n",
    "    \"\"\"\n",
    "    Paso 5) Regresar al Dataset original, y remover todas aquellas palabras que contengan un valor de idf menor\n",
    "    al promedio de TODOS los idfs de la tabla obtenidaa\n",
    "    \"\"\"\n",
    "    analysis.filter_below_average_idf()\n",
    "\n",
    "    \"\"\"\n",
    "    Paso 6)\n",
    "    Generar nuevamente la tabla de idf a partir de valores de idf para los textos filtrados y mostrarla en pantalla\n",
    "    \"\"\"\n",
    "    print(\"\\nFiltered IDF Table:\")\n",
    "    analysis.calculate_filtered_idf()\n",
    "    print(analysis.filtered_df)\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    Paso 7) Imprimir en pantalla el top de las 10 palabras MAS relevantes, y el top de las 20 palabras MENOS \n",
    "    relevantes\n",
    "    \"\"\"\n",
    "    top, bottom = analysis.top_bottom_words()\n",
    "    print(\"\\nTop 10 Words:\")\n",
    "    print(top)\n",
    "    print(\"\\nBottom 20 Words:\")\n",
    "    print(bottom)\n",
    "\n",
    "    # TODO discover UNIX encoding\n",
    "    # FIXME all have the same IDF values\n",
    "    # TODO add docstrings"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
