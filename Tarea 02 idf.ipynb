{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N921vKRx4RIH"
   },
   "source": [
    "# Tarea 02: idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nd_x9Fv4rMre"
   },
   "source": [
    "### En esta tarea, se trabajará con un corpus de textos para obtener las palabras mas y menos relevantes a partir de un análisis por tf-idf. Deberás seguir los pasos de este notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPaso 1) \\nDel archivo adjunto (Questions.csv), extraer las primeras 2,000 preguntas presentes en la columna \"Title\", \\ny generar un nuevo archivo que SOLO contenga dichas preguntas.\\nNOTA: El archivo es muy grande, por lo que deberás de procesarlo en tu equipo local (No usar colab), y \\ngenerar con ayuda de un script el nuevo archivo, si llegas a tener problemas con el tipo de codificación\\ny se generan caracteres raros, deberás resolverlo con la codificación adecuada de lectura\\n\\nPaso 2) \\nTraducirlas las preguntas del nuevo archivo al idioma Español (Investiga cómo se realiza el proceso de \\ntraducción automático utilizando Google Sheets y Google Translate)\\nSe deberá de agregar una nueva columna al archivo .csv llamada \"Textos_traducidos\" donde se incluirán las \\ntraducciones de los textos originales (Incluir este nuevo .csv en la entrega de tu tarea)\\n\\nPaso 3) \\nCon este nuevo .csv de 2,000 textos traducidos al español, crear un dataset (de 2,000 textos) y aplicar \\nOBLIGATORIAMENTE los siguientes preprocesamientos:\\n- Lematización de todas las palabras\\n- Filtrado de StopWords\\n- Pasar todo a minúsculas\\n\\nPaso 4) \\nDespués de haber limpiado el dataset anterior, generar el vector de idf correspondiente a TODOS los textos\\ny mostrarlo en pantalla\\n\\nPaso 5) Regresar al Dataset original, y remover todas aquellas palabras que contengan un valor de idf menor\\nal promedio de TODOS los idfs de la tabla obtenida\\n\\nPaso 6)\\nGenerar nuevamente la tabla de idf a partir de valores de idf para los textos filtrados y mostrarla en pantalla\\n\\nPaso 7) Imprimir en pantalla el top de las 10 palabras MAS relevantes, y el top de las 20 palabras MENOS \\nrelevantes\\n\\nNOTA: Recuerda que deberás de entregar 2 archivos, un .csv con los 2,000 textos originales y sus traducciones\\ny un .ipynb con todo el procedimiento realizado con sus respectivos comentarios, y DocStrings\\nIMPORTANTE: Todo el proceso deberá realizarse por medio de métodos, NO se aceptará programación estructurada,\\npor lo que, por ejemplo, deberá haber un método para filtrar StopWords, otro para obtener el promedio de \\nidf de todo el conjunto de palabras, etc. \\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Paso 1) \n",
    "Del archivo adjunto (Questions.csv), extraer las primeras 2,000 preguntas presentes en la columna \"Title\", \n",
    "y generar un nuevo archivo que SOLO contenga dichas preguntas.\n",
    "NOTA: El archivo es muy grande, por lo que deberás de procesarlo en tu equipo local (No usar colab), y \n",
    "generar con ayuda de un script el nuevo archivo, si llegas a tener problemas con el tipo de codificación\n",
    "y se generan caracteres raros, deberás resolverlo con la codificación adecuada de lectura\n",
    "\n",
    "Paso 2) \n",
    "Traducirlas las preguntas del nuevo archivo al idioma Español (Investiga cómo se realiza el proceso de \n",
    "traducción automático utilizando Google Sheets y Google Translate)\n",
    "Se deberá de agregar una nueva columna al archivo .csv llamada \"Textos_traducidos\" donde se incluirán las \n",
    "traducciones de los textos originales (Incluir este nuevo .csv en la entrega de tu tarea)\n",
    "\n",
    "Paso 3) \n",
    "Con este nuevo .csv de 2,000 textos traducidos al español, crear un dataset (de 2,000 textos) y aplicar \n",
    "OBLIGATORIAMENTE los siguientes preprocesamientos:\n",
    "- Lematización de todas las palabras\n",
    "- Filtrado de StopWords\n",
    "- Pasar todo a minúsculas\n",
    "\n",
    "Paso 4) \n",
    "Después de haber limpiado el dataset anterior, generar el vector de idf correspondiente a TODOS los textos\n",
    "y mostrarlo en pantalla\n",
    "\n",
    "Paso 5) Regresar al Dataset original, y remover todas aquellas palabras que contengan un valor de idf menor\n",
    "al promedio de TODOS los idfs de la tabla obtenida\n",
    "\n",
    "Paso 6)\n",
    "Generar nuevamente la tabla de idf a partir de valores de idf para los textos filtrados y mostrarla en pantalla\n",
    "\n",
    "Paso 7) Imprimir en pantalla el top de las 10 palabras MAS relevantes, y el top de las 20 palabras MENOS \n",
    "relevantes\n",
    "\n",
    "NOTA: Recuerda que deberás de entregar 2 archivos, un .csv con los 2,000 textos originales y sus traducciones\n",
    "y un .ipynb con todo el procedimiento realizado con sus respectivos comentarios, y DocStrings\n",
    "IMPORTANTE: Todo el proceso deberá realizarse por medio de métodos, NO se aceptará programación estructurada,\n",
    "por lo que, por ejemplo, deberá haber un método para filtrar StopWords, otro para obtener el promedio de \n",
    "idf de todo el conjunto de palabras, etc. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "7InFgho6rytt"
   },
   "outputs": [],
   "source": [
    "# Recuerda que todos los métodos que utilices deberán de contar con el formato DocString\n",
    "# como en el ejemplo que se muestra a continuación:\n",
    "# Ejemplo de formato Docstrings:\n",
    "\n",
    "# def NombreFuncion(arg1, arg2, arg3):\n",
    "#     \"\"\"\n",
    "#     Este método sirve para... utilizando... y devuelve...\n",
    "    \n",
    "#     Args:\n",
    "#         string arg1: Esta es una cadena de texto que...\n",
    "#         int arg 2: Es un número entero que se usa para...\n",
    "#         dict arg 3: Diccionario que sirve para...\n",
    "\n",
    "#     Returns:\n",
    "#         string: Cadena del texto ya corregido...\n",
    "#         int: El la cantidad de correcciones realizadas...\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Aquí debe de ir la lógica de la función (Después de la documentación)\n",
    "#     Texto = \"\"\n",
    "#     corr = 5\n",
    "    \n",
    "#   return Texto, corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mchardet\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mQuestions.csv\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m----> 4\u001b[0m     result \u001b[39m=\u001b[39m chardet\u001b[39m.\u001b[39;49mdetect(f\u001b[39m.\u001b[39;49mread())\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.9/site-packages/chardet/__init__.py:49\u001b[0m, in \u001b[0;36mdetect\u001b[0;34m(byte_str, should_rename_legacy)\u001b[0m\n\u001b[1;32m     47\u001b[0m     byte_str \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(byte_str)\n\u001b[1;32m     48\u001b[0m detector \u001b[39m=\u001b[39m UniversalDetector(should_rename_legacy\u001b[39m=\u001b[39mshould_rename_legacy)\n\u001b[0;32m---> 49\u001b[0m detector\u001b[39m.\u001b[39;49mfeed(byte_str)\n\u001b[1;32m     50\u001b[0m \u001b[39mreturn\u001b[39;00m detector\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.9/site-packages/chardet/universaldetector.py:274\u001b[0m, in \u001b[0;36mUniversalDetector.feed\u001b[0;34m(self, byte_str)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_charset_probers\u001b[39m.\u001b[39mappend(MacRomanProber())\n\u001b[1;32m    273\u001b[0m \u001b[39mfor\u001b[39;00m prober \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_charset_probers:\n\u001b[0;32m--> 274\u001b[0m     \u001b[39mif\u001b[39;00m prober\u001b[39m.\u001b[39;49mfeed(byte_str) \u001b[39m==\u001b[39m ProbingState\u001b[39m.\u001b[39mFOUND_IT:\n\u001b[1;32m    275\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresult \u001b[39m=\u001b[39m {\n\u001b[1;32m    276\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m\"\u001b[39m: prober\u001b[39m.\u001b[39mcharset_name,\n\u001b[1;32m    277\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mconfidence\u001b[39m\u001b[39m\"\u001b[39m: prober\u001b[39m.\u001b[39mget_confidence(),\n\u001b[1;32m    278\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mlanguage\u001b[39m\u001b[39m\"\u001b[39m: prober\u001b[39m.\u001b[39mlanguage,\n\u001b[1;32m    279\u001b[0m         }\n\u001b[1;32m    280\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.9/site-packages/chardet/charsetgroupprober.py:70\u001b[0m, in \u001b[0;36mCharSetGroupProber.feed\u001b[0;34m(self, byte_str)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m prober\u001b[39m.\u001b[39mactive:\n\u001b[1;32m     69\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m state \u001b[39m=\u001b[39m prober\u001b[39m.\u001b[39;49mfeed(byte_str)\n\u001b[1;32m     71\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m state:\n\u001b[1;32m     72\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.9/site-packages/chardet/sbcharsetprober.py:97\u001b[0m, in \u001b[0;36mSingleByteCharSetProber.feed\u001b[0;34m(self, byte_str)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed\u001b[39m(\u001b[39mself\u001b[39m, byte_str: Union[\u001b[39mbytes\u001b[39m, \u001b[39mbytearray\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ProbingState:\n\u001b[1;32m     95\u001b[0m     \u001b[39m# TODO: Make filter_international_words keep things in self.alphabet\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model\u001b[39m.\u001b[39mkeep_ascii_letters:\n\u001b[0;32m---> 97\u001b[0m         byte_str \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilter_international_words(byte_str)\n\u001b[1;32m     98\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m         byte_str \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mremove_xml_tags(byte_str)\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.9/site-packages/chardet/charsetprober.py:94\u001b[0m, in \u001b[0;36mCharSetProber.filter_international_words\u001b[0;34m(buf)\u001b[0m\n\u001b[1;32m     89\u001b[0m filtered \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m()\n\u001b[1;32m     91\u001b[0m \u001b[39m# This regex expression filters out only words that have at-least one\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39m# international character. The word may include one marker character at\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39m# the end.\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m words \u001b[39m=\u001b[39m INTERNATIONAL_WORDS_PATTERN\u001b[39m.\u001b[39;49mfindall(buf)\n\u001b[1;32m     96\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words:\n\u001b[1;32m     97\u001b[0m     filtered\u001b[39m.\u001b[39mextend(word[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "with open(\"Questions.csv\", 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "with open(\"TranslatedQuestions.csv\", 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished extracting the first 2000 rows of the column 'Title' and saved to 'Questions_2000.csv'.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Paso 1) \n",
    "Del archivo adjunto (Questions.csv), extraer las primeras 2,000 preguntas presentes en la columna \"Title\", \n",
    "y generar un nuevo archivo que SOLO contenga dichas preguntas.\n",
    "NOTA: El archivo es muy grande, por lo que deberás de procesarlo en tu equipo local (No usar colab), y \n",
    "generar con ayuda de un script el nuevo archivo, si llegas a tener problemas con el tipo de codificación\n",
    "y se generan caracteres raros, deberás resolverlo con la codificación adecuada de lectura\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def extract_and_save_titles(input_file, output_file, num_rows=2000):\n",
    "    \"\"\"\n",
    "    Extract a specified number of rows from the \"Title\" column of a CSV file and save to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        input_file (str): Path to the input CSV file.\n",
    "        output_file (str): Path to save the extracted titles as a new CSV file.\n",
    "        num_rows (int, optional): Number of rows to extract from the \"Title\" column. Default is 2000.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_file, encoding='latin1')\n",
    "    df_titles = pd.DataFrame(df.head(num_rows)[\"Title\"])\n",
    "    df_titles.to_csv(output_file, index=False)\n",
    "    print(\n",
    "        f\"Finished extracting the first {num_rows} rows of the column 'Title' and saved to '{output_file}'.\"\n",
    "    )\n",
    "\n",
    "\n",
    "extract_and_save_titles(\"Questions.csv\", \"Questions_2000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0  How can I find the full path to a font from it...   \n",
      "1            Get a preview JPEG of a PDF on Windows?   \n",
      "2  Continuous Integration System for a Python Cod...   \n",
      "3     cx_Oracle: How do I iterate over a result set?   \n",
      "4  Using 'in' to match an attribute of Python obj...   \n",
      "\n",
      "                                    Translated_Texts  \n",
      "0  ¿Cómo puedo encontrar el camino completo a una...  \n",
      "1  ¿Obtener una vista previa JPEG de un PDF en Wi...  \n",
      "2  Sistema de integración continua para una base ...  \n",
      "3  CX_Oracle: ¿Cómo itero sobre un conjunto de re...  \n",
      "4  Usar 'in' para que coincida con un atributo de...  \n",
      "\n",
      "-----------\n",
      "FINISHED TRANSLATING\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Paso 2) \n",
    "Traducirlas las preguntas del nuevo archivo al idioma Español (Investiga cómo se realiza el proceso de \n",
    "traducción automático utilizando Google Sheets y Google Translate)\n",
    "Se deberá de agregar una nueva columna al archivo .csv llamada \"Textos_traducidos\" donde se incluirán las \n",
    "traducciones de los textos originales (Incluir este nuevo .csv en la entrega de tu tarea)\n",
    "\"\"\"\n",
    "df = pd.read_csv(\"TranslatedQuestions.csv\", encoding=\"utf-8\")\n",
    "print(df.head())\n",
    "print(\"\\n-----------\\nFINISHED TRANSLATING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/eubgo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/eubgo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/eubgo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Paso 3) \n",
    "Con este nuevo .csv de 2,000 textos traducidos al español, crear un dataset (de 2,000 textos) y aplicar \n",
    "OBLIGATORIAMENTE los siguientes preprocesamientos:\n",
    "- Lematización de todas las palabras\n",
    "- Filtrado de StopWords\n",
    "- Pasar todo a minúsculas\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "df = pd.read_csv(\"TranslatedQuestions.csv\", encoding=\"utf-8\")[\"Translated_Texts\"]\n",
    "# print(df.head())\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = nltk.corpus.stopwords.words(\"spanish\")\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    words = [word.lower() for word in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "df = df.apply(preprocess_text)\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Word  IDF Value\n",
      "0       000z   7.908255\n",
      "1         04   7.908255\n",
      "2       0x1a   7.908255\n",
      "3         10   7.215108\n",
      "4        100   7.908255\n",
      "...      ...        ...\n",
      "3275   única   6.991964\n",
      "3276  únicas   7.908255\n",
      "3277   único   7.908255\n",
      "3278  únicos   7.908255\n",
      "3279    útil   7.502790\n",
      "\n",
      "[3280 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Paso 4) \n",
    "Después de haber limpiado el dataset anterior, generar el vector de idf correspondiente a TODOS los textos\n",
    "y mostrarlo en pantalla\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "tfidf_matrix = vectorizer.fit_transform(df)\n",
    "features = vectorizer.get_feature_names_out()\n",
    "idf = vectorizer.idf_\n",
    "idf_dict = dict(zip(features, idf))\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "idf_df = pd.DataFrame(list(idf_dict.items()), columns=[\"Word\", \"IDF Value\"])\n",
    "print(idf_df)\n",
    "\n",
    "top_3_words = sorted(idf_dict.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "bottom_3_words = sorted(idf_dict.items(), key=lambda x: x[1])[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "average_idf = np.mean(idf_df[\"IDF Value\"])\n",
    "\n",
    "\n",
    "def filter_words_by_idf(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if idf_dict.get(word, 0) >= average_idf]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "df_filtered = df.apply(filter_words_by_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Word  IDF Value\n",
      "0       0x1a   7.908255\n",
      "1        100   7.908255\n",
      "2       1000   7.908255\n",
      "3        104   7.908255\n",
      "4       1123   7.908255\n",
      "...      ...        ...\n",
      "1758  índice   7.908255\n",
      "1759  último   7.908255\n",
      "1760  únicas   7.908255\n",
      "1761   único   7.908255\n",
      "1762  únicos   7.908255\n",
      "\n",
      "[1763 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Paso 6)\n",
    "Generar nuevamente la tabla de idf a partir de valores de idf para los textos filtrados y mostrarla en pantalla\n",
    "\"\"\"\n",
    "\n",
    "tfidf_matrix_filtered = vectorizer.fit_transform(df_filtered)\n",
    "features_filtered = vectorizer.get_feature_names_out()\n",
    "idf_filtered = vectorizer.idf_\n",
    "idf_dict_filtered = dict(zip(features_filtered, idf_filtered))\n",
    "\n",
    "idf_df_filtered = pd.DataFrame(\n",
    "    list(idf_dict_filtered.items()), columns=[\"Word\", \"IDF Value\"]\n",
    ")\n",
    "print(idf_df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 palabras más relevantes:\n",
      "                Word  IDF Value\n",
      "0               0x1a   7.908255\n",
      "1171       paralelos   7.908255\n",
      "1182       partición   7.908255\n",
      "1181         parsing   7.908255\n",
      "1180          parsec   7.908255\n",
      "1179         parecen   7.908255\n",
      "1178       parciales   7.908255\n",
      "1177         parcial   7.908255\n",
      "1176          parche   7.908255\n",
      "1175  parametrizadas   7.908255\n",
      "\n",
      "\n",
      "Top 20 palabras menos relevantes:\n",
      "              Word  IDF Value\n",
      "599       escanear   7.908255\n",
      "598         escala   7.908255\n",
      "597      escabeche   7.908255\n",
      "596       erróneas   7.908255\n",
      "595   equivalentes   7.908255\n",
      "594   equivalencia   7.908255\n",
      "593          envíe   7.908255\n",
      "592          envía   7.908255\n",
      "591       envuelvo   7.908255\n",
      "590      envoltura   7.908255\n",
      "589     envoltorio   7.908255\n",
      "588     enviroment   7.908255\n",
      "587       enviarlo   7.908255\n",
      "586      entrewiki   7.908255\n",
      "585       entrante   7.908255\n",
      "584          enter   7.908255\n",
      "583       entender   7.908255\n",
      "582   enrutamiento   7.908255\n",
      "581    enriquecido   7.908255\n",
      "1762        únicos   7.908255\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Paso 7) Imprimir en pantalla el top de las 10 palabras MAS relevantes, y el top de las 20 palabras MENOS \n",
    "relevantes\n",
    "\"\"\"\n",
    "idf_df_filtered = idf_df_filtered.sort_values(by=\"IDF Value\", ascending=False)\n",
    "\n",
    "\n",
    "top_10_words = idf_df_filtered.head(10)\n",
    "print(\"Top 10 palabras más relevantes:\")\n",
    "print(top_10_words)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "bottom_20_words = idf_df_filtered.tail(20)\n",
    "print(\"Top 20 palabras menos relevantes:\")\n",
    "print(bottom_20_words)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
