{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N921vKRx4RIH"
   },
   "source": [
    "# Tarea 02: idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nd_x9Fv4rMre"
   },
   "source": [
    "### En esta tarea, se trabajará con un corpus de textos para obtener las palabras mas y menos relevantes a partir de un análisis por tf-idf. Deberás seguir los pasos de este notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T20:29:01.811966190Z",
     "start_time": "2023-09-08T20:29:01.660019697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPaso 1) \\nDel archivo adjunto (Questions.csv), extraer las primeras 2,000 preguntas presentes en la columna \"Title\", \\ny generar un nuevo archivo que SOLO contenga dichas preguntas.\\nNOTA: El archivo es muy grande, por lo que deberás de procesarlo en tu equipo local (No usar colab), y \\ngenerar con ayuda de un script el nuevo archivo, si llegas a tener problemas con el tipo de codificación\\ny se generan caracteres raros, deberás resolverlo con la codificación adecuada de lectura\\n\\nPaso 2) \\nTraducirlas las preguntas del nuevo archivo al idioma Español (Investiga cómo se realiza el proceso de \\ntraducción automático utilizando Google Sheets y Google Translate)\\nSe deberá de agregar una nueva columna al archivo .csv llamada \"Textos_traducidos\" donde se incluirán las \\ntraducciones de los textos originales (Incluir este nuevo .csv en la entrega de tu tarea)\\n\\nPaso 3) \\nCon este nuevo .csv de 2,000 textos traducidos al español, crear un dataset (de 2,000 textos) y aplicar \\nOBLIGATORIAMENTE los siguientes preprocesamientos:\\n- Lematización de todas las palabras\\n- Filtrado de StopWords\\n- Pasar todo a minúsculas\\n\\nPaso 4) \\nDespués de haber limpiado el dataset anterior, generar el vector de idf correspondiente a TODOS los textos\\ny mostrarlo en pantalla\\n\\nPaso 5) Regresar al Dataset original, y remover todas aquellas palabras que contengan un valor de idf menor\\nal promedio de TODOS los idfs de la tabla obtenida\\n\\nPaso 6)\\nGenerar nuevamente la tabla de idf a partir de valores de idf para los textos filtrados y mostrarla en pantalla\\n\\nPaso 7) Imprimir en pantalla el top de las 10 palabras MAS relevantes, y el top de las 20 palabras MENOS \\nrelevantes\\n\\nNOTA: Recuerda que deberás de entregar 2 archivos, un .csv con los 2,000 textos originales y sus traducciones\\ny un .ipynb con todo el procedimiento realizado con sus respectivos comentarios, y DocStrings\\nIMPORTANTE: Todo el proceso deberá realizarse por medio de métodos, NO se aceptará programación estructurada,\\npor lo que, por ejemplo, deberá haber un método para filtrar StopWords, otro para obtener el promedio de \\nidf de todo el conjunto de palabras, etc. \\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Paso 1) \n",
    "Del archivo adjunto (Questions.csv), extraer las primeras 2,000 preguntas presentes en la columna \"Title\", \n",
    "y generar un nuevo archivo que SOLO contenga dichas preguntas.\n",
    "NOTA: El archivo es muy grande, por lo que deberás de procesarlo en tu equipo local (No usar colab), y \n",
    "generar con ayuda de un script el nuevo archivo, si llegas a tener problemas con el tipo de codificación\n",
    "y se generan caracteres raros, deberás resolverlo con la codificación adecuada de lectura\n",
    "\n",
    "Paso 2) \n",
    "Traducirlas las preguntas del nuevo archivo al idioma Español (Investiga cómo se realiza el proceso de \n",
    "traducción automático utilizando Google Sheets y Google Translate)\n",
    "Se deberá de agregar una nueva columna al archivo .csv llamada \"Textos_traducidos\" donde se incluirán las \n",
    "traducciones de los textos originales (Incluir este nuevo .csv en la entrega de tu tarea)\n",
    "\n",
    "Paso 3) \n",
    "Con este nuevo .csv de 2,000 textos traducidos al español, crear un dataset (de 2,000 textos) y aplicar \n",
    "OBLIGATORIAMENTE los siguientes preprocesamientos:\n",
    "- Lematización de todas las palabras\n",
    "- Filtrado de StopWords\n",
    "- Pasar todo a minúsculas\n",
    "\n",
    "Paso 4) \n",
    "Después de haber limpiado el dataset anterior, generar el vector de idf correspondiente a TODOS los textos\n",
    "y mostrarlo en pantalla\n",
    "\n",
    "Paso 5) Regresar al Dataset original, y remover todas aquellas palabras que contengan un valor de idf menor\n",
    "al promedio de TODOS los idfs de la tabla obtenida\n",
    "\n",
    "Paso 6)\n",
    "Generar nuevamente la tabla de idf a partir de valores de idf para los textos filtrados y mostrarla en pantalla\n",
    "\n",
    "Paso 7) Imprimir en pantalla el top de las 10 palabras MAS relevantes, y el top de las 20 palabras MENOS \n",
    "relevantes\n",
    "\n",
    "NOTA: Recuerda que deberás de entregar 2 archivos, un .csv con los 2,000 textos originales y sus traducciones\n",
    "y un .ipynb con todo el procedimiento realizado con sus respectivos comentarios, y DocStrings\n",
    "IMPORTANTE: Todo el proceso deberá realizarse por medio de métodos, NO se aceptará programación estructurada,\n",
    "por lo que, por ejemplo, deberá haber un método para filtrar StopWords, otro para obtener el promedio de \n",
    "idf de todo el conjunto de palabras, etc. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T20:29:01.812178422Z",
     "start_time": "2023-09-08T20:29:01.702328244Z"
    },
    "id": "7InFgho6rytt"
   },
   "outputs": [],
   "source": [
    "# Recuerda que todos los métodos que utilices deberán de contar con el formato DocString\n",
    "# como en el ejemplo que se muestra a continuación:\n",
    "# Ejemplo de formato Docstrings:\n",
    "\n",
    "# def NombreFuncion(arg1, arg2, arg3):\n",
    "#     \"\"\"\n",
    "#     Este método sirve para... utilizando... y devuelve...\n",
    "    \n",
    "#     Args:\n",
    "#         string arg1: Esta es una cadena de texto que...\n",
    "#         int arg 2: Es un número entero que se usa para...\n",
    "#         dict arg 3: Diccionario que sirve para...\n",
    "\n",
    "#     Returns:\n",
    "#         string: Cadena del texto ya corregido...\n",
    "#         int: El la cantidad de correcciones realizadas...\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Aquí debe de ir la lógica de la función (Después de la documentación)\n",
    "#     Texto = \"\"\n",
    "#     corr = 5\n",
    "    \n",
    "#   return Texto, corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T20:29:01.812555740Z",
     "start_time": "2023-09-08T20:29:01.702558405Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/eubgo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/eubgo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/eubgo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T20:29:01.812684352Z",
     "start_time": "2023-09-08T20:29:01.702765799Z"
    }
   },
   "outputs": [],
   "source": [
    "def _clean_char_tokens(text):\n",
    "    \"\"\"\n",
    "    Removes unwanted characters from the tokenized words in the text.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The tokenized text to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "    - str: A string with cleaned tokens.\n",
    "    \"\"\"\n",
    "    words = nltk.word_tokenize(text)\n",
    "    cleaned_tokens = [re.sub(r\"^\\W+|\\W*$\", \"\", word) for word in words if len(word) > 1]\n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "\n",
    "class TextAnalysis:\n",
    "    def __init__(self, filepath, flag):\n",
    "        \"\"\"\n",
    "        Initialize the TextAnalysis object.\n",
    "\n",
    "        Parameters:\n",
    "        - filepath (str): Path to the input CSV file containing 'Translated_Texts' column.\n",
    "        - flag (bool): Flag to determine whether to remove words with below-average IDF values. \n",
    "                       If set to True, the words are kept; if False, they are removed.\n",
    "        \"\"\"\n",
    "        self.df_above_avg = None\n",
    "        self.df_sorted = None\n",
    "        self.average_idf = None\n",
    "        self.idf_values = None\n",
    "        self.df = None\n",
    "        self.filepath = filepath\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = nltk.corpus.stopwords.words(\"spanish\")\n",
    "\n",
    "        self._initialize_analysis(flag)\n",
    "\n",
    "    def _initialize_analysis(self, flag):\n",
    "        \"\"\"\n",
    "        A helper function to sequentially execute the steps required for the text analysis.\n",
    "        \n",
    "        Parameters:\n",
    "        - flag (bool): Flag to determine if below-average IDF words need to be removed.\n",
    "        \"\"\"\n",
    "        self._read_data()\n",
    "        self._preprocess_translated_text()\n",
    "        self._calculate_idf()\n",
    "\n",
    "        if not flag:\n",
    "            self._remove_below_average_words_from_sentences()\n",
    "\n",
    "    def _read_data(self):\n",
    "        \"\"\"\n",
    "        Read the CSV data from the provided filepath and saves the 'Translated_Texts' column into a dataframe.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(self.filepath)[\"Translated_Texts\"].dropna()\n",
    "\n",
    "    def _process_text(self, text):\n",
    "        \"\"\"\n",
    "        Process a given text - tokenizes, lemmatizes, and removes stopwords.\n",
    "        \n",
    "        Parameters:\n",
    "        - text (str): The input text to be processed.\n",
    "\n",
    "        Returns:\n",
    "        - str: A string with processed text.\n",
    "        \"\"\"\n",
    "        words = nltk.word_tokenize(text)\n",
    "        words = [\n",
    "            self.lemmatizer.lemmatize(word.lower())\n",
    "            for word in words\n",
    "            if word.lower() not in self.stop_words and not (any(c.isalpha() for c in word) and any(c.isdigit() for c in word))\n",
    "        ]\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def _preprocess_translated_text(self):\n",
    "        \"\"\"\n",
    "        Apply the text processing and token cleaning functions to the dataframe.\n",
    "        \"\"\"\n",
    "        self.df = self.df.apply(self._process_text).apply(_clean_char_tokens).dropna()\n",
    "\n",
    "    def _calculate_idf(self):\n",
    "        \"\"\"\n",
    "        Calculate the Inverse Document Frequency (IDF) values of words present in the dataset. \n",
    "        Initializes the idf_values, average_idf, and df_sorted attributes of the class.\n",
    "        \"\"\"\n",
    "        vectorizer = TfidfVectorizer(use_idf=True)\n",
    "        vectorizer.fit_transform(self.df)\n",
    "        self.idf_values = pd.DataFrame({\n",
    "            \"Word\": vectorizer.get_feature_names_out(),\n",
    "            \"IDF\": vectorizer.idf_\n",
    "        })\n",
    "        self.average_idf = self.idf_values[\"IDF\"].mean()\n",
    "        self.df_sorted = self.idf_values.sort_values(by='IDF', ascending=False)\n",
    "\n",
    "    def _filter_below_average_idf(self):\n",
    "        \"\"\"\n",
    "        Filter out words with IDF values below the average IDF value.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of words with below-average IDF values.\n",
    "        \"\"\"\n",
    "        below_average = self.idf_values[self.idf_values[\"IDF\"] < self.average_idf]\n",
    "        return below_average[\"Word\"].str.lower().tolist()\n",
    "\n",
    "    def _remove_below_average_words_from_sentences(self):\n",
    "        \"\"\"\n",
    "        Removes words with below average IDF values from the sentences in the dataframe.\n",
    "        \"\"\"\n",
    "        below_average_words = self._filter_below_average_idf()\n",
    "        self.df_above_avg = self.df.apply(lambda text: \" \".join([word for word in text.split() if word.lower() not in below_average_words]))\n",
    "        series_idf_table(self.df_above_avg)\n",
    "\n",
    "\n",
    "def series_idf_table(series):\n",
    "    \"\"\"\n",
    "    Calculate and display the IDF for each word in the given pandas Series.\n",
    "\n",
    "    Parameters:\n",
    "    - series (Series): A pandas Series where each entry is a document.\n",
    "\n",
    "    Prints:\n",
    "    - Initial and filtered IDF tables showing top and bottom IDF values.\n",
    "    \"\"\"\n",
    "    print(\"Filtered IDF Table:\")\n",
    "    tokenized_docs = series.apply(lambda x: x.split() if isinstance(x, str) else [])\n",
    "    doc_freq = Counter(word for doc in tokenized_docs for word in set(doc))\n",
    "    n = len(tokenized_docs)\n",
    "\n",
    "    idf_values = {word: np.log(n / count) for word, count in doc_freq.items()}\n",
    "    result = pd.Series(idf_values).sort_values(ascending=False)\n",
    "    df = result.reset_index()\n",
    "    df.columns = ['Word', 'IDF']\n",
    "    print(df)\n",
    "\n",
    "    display_top_bottom_idf(result)\n",
    "\n",
    "\n",
    "def display_top_bottom_idf(data, top=10, bottom=10):\n",
    "    \"\"\"\n",
    "    Prints the top and bottom words based on their IDF values.\n",
    "\n",
    "    Parameters:\n",
    "    - data (Series): A pandas Series containing IDF values indexed by words.\n",
    "    - top (int, optional): Number of top words to display. Default is 10.\n",
    "    - bottom (int, optional): Number of bottom words to display. Default is 10.\n",
    "\n",
    "    Prints:\n",
    "    - Top and bottom words based on IDF values.\n",
    "    \"\"\"\n",
    "    print(\"\\nTop {} words:\".format(top))\n",
    "    print(\"Word\".ljust(20), \"IDF Value\")\n",
    "    print(\"-\" * 30)\n",
    "    for word, idf in data.head(top).items():\n",
    "        print(word.ljust(20), idf)\n",
    "\n",
    "    print(\"\\nBottom {} words:\".format(bottom))\n",
    "    print(\"Word\".ljust(20), \"IDF Value\")\n",
    "    print(\"-\" * 30)\n",
    "    for word, idf in data.tail(bottom).items():\n",
    "        print(word.ljust(20), idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T20:29:01.812809810Z",
     "start_time": "2023-09-08T20:29:01.703045544Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_and_save_titles(input_file, output_file, num_rows=2000):\n",
    "    \"\"\"\n",
    "    Extract a specified number of rows from the \"Title\" column of a CSV file and save to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        input_file (str): Path to the input CSV file.\n",
    "        output_file (str): Path to save the extracted titles as a new CSV file.\n",
    "        num_rows (int, optional): Number of rows to extract from the \"Title\" column. Default is 2000.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_file, encoding=\"latin-1\")\n",
    "    df_titles = pd.DataFrame(df.head(num_rows)[\"Title\"])\n",
    "    df_titles.to_csv(output_file, index=False, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T20:29:11.153730245Z",
     "start_time": "2023-09-08T20:29:01.703198243Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture captura\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Paso 1)\n",
    "    Del archivo adjunto (Questions.csv), extraer las primeras 2,000 preguntas presentes en la columna \"Title\",\n",
    "    y generar un nuevo archivo que SOLO contenga dichas preguntas.\n",
    "    NOTA: El archivo es muy grande, por lo que deberás de procesarlo en tu equipo local (No usar colab), y\n",
    "    generar con ayuda de un script el nuevo archivo, si llegas a tener problemas con el tipo de codificación\n",
    "    y se generan caracteres raros, deberás resolverlo con la codificación adecuada de lectura\n",
    "    \"\"\"\n",
    "    extract_and_save_titles(\"Questions.csv\", \"Questions_2000.csv\")\n",
    "\n",
    "    \"\"\"\n",
    "    Paso 2) \n",
    "    Traducirlas las preguntas del nuevo archivo al idioma Español (Investiga cómo se realiza el proceso de \n",
    "    traducción automático utilizando Google Sheets y Google Translate)\n",
    "    Se deberá de agregar una nueva columna al archivo .csv llamada \"Textos_traducidos\" donde se incluirán las \n",
    "    traducciones de los textos originales (Incluir este nuevo .csv en la entrega de tu tarea)\n",
    "    \"\"\"\n",
    "    # In TranslatedQuestions.csv file\n",
    "\n",
    "    \"\"\"\n",
    "    Paso 3) \n",
    "    Con este nuevo .csv de 2,000 textos traducidos al español, crear un dataset (de 2,000 textos) y aplicar \n",
    "    OBLIGATORIAMENTE los siguientes preprocesamientos:\n",
    "    - Lematización de todas las palabras\n",
    "    - Filtrado de StopWords\n",
    "    - Pasar todo a minúsculas\n",
    "    \"\"\"\n",
    "    analysis = TextAnalysis(\"TranslatedQuestions.csv\", 1)\n",
    "\n",
    "    \"\"\"\n",
    "    Paso 4) \n",
    "    Después de haber limpiado el dataset anterior, generar el vector de idf correspondiente a TODOS los textos\n",
    "    y mostrarlo en pantalla\n",
    "    \"\"\"\n",
    "    print(\"Initial IDF Table:\")\n",
    "    print(analysis.df_sorted)\n",
    "    \n",
    "    \"\"\"\n",
    "    Paso 5) Regresar al Dataset original, y remover todas aquellas palabras que contengan un valor de idf menor\n",
    "    al promedio de TODOS los idfs de la tabla obtenida\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Paso 6)\n",
    "    Generar nuevamente la tabla de idf a partir de valores de idf para los textos filtrados y mostrarla en pantalla\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Paso 7) Imprimir en pantalla el top de las 10 palabras MAS relevantes, y el top de las 20 palabras MENOS \n",
    "    relevantes\n",
    "    \"\"\"\n",
    "    new_anaylsis = TextAnalysis(\"TranslatedQuestions.csv\", 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T20:29:11.194071249Z",
     "start_time": "2023-09-08T20:29:11.151746627Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"output.txt\", \"w\") as f:\n",
    "    f.write(captura.stdout)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
